\chapter{Wyniki badañ eksperymentalnych}

\section{Eksperyment on-line - wydajnoœæ systemu}
\subsection{Opis scenariusza i przebiegu testów}
W trakcie przeprowadzania testów w symulatorze CARLA,  dzia³anie programu zosta³o sprawdzone poprzez wykonanie serii przejazdów po okreœlonej trasie z widoku pierwszej osoby. Ka¿dy przejazd by³ nagrywany a nastêpnie w trzech uprzednio wybranych miejscach wykonywany by³ zrzut ekranu. Testy obejmowa³y wszelkie mo¿liwe kombinacje nastêpuj¹cych scenariuszy:

\begin{itemize}
	\item \textbf{Przejazdy dla ró¿nych pór dnia:} testy przeprowadzono zarówno w ci¹gu dnia, jak i w nocy, aby oceniæ wp³yw oœwietlenia na przebieg symulacji.
	\item \textbf{Przejazdy dla ró¿nych warunków pogodowych:} badania obejmowa³y symulacje w ró¿nych warunkach atmosferycznych, takich jak s³oñce oraz deszcz, co pozwoli³o na sprawdzenie, jak zmienia siê zachowanie pojazdu i wizualizacja symulacji w tych warunkach.
	\item \textbf{Przejazdy przy ró¿nych poziomach nasycenia ruchu:} testy wykonywano przy ró¿nych poziomach natê¿enia ruchu samochodowego i pieszego (ma³y, œredni, du¿y), aby oceniæ, jak system radzi sobie w ró¿nych scenariuszach natê¿enia ruchu.
	\item \textbf{Przejazdy dla ró¿nych modeli:} badania zawiera³y równie¿ przetestowanie dla du¿ego oraz ma³ego modelu YOLO.
\end{itemize}

Podczas testowania serwer uruchomiony by³ na GPU, natomiast klient a tym samym program w³aœciwy na CPU. Wynika to z faktu, i¿ zasoby karty graficznej by³y zajête przez symulator CARLA, przez co nie by³o mo¿liwoœci uruchomienia równoczeœnie klienta wraz z YOLO.

Poni¿ej przedstawiono trzy punkty, w których dokonywane by³y pomiary skutecznoœci funkcjonowania programu podczas jazdy. Na jego podstawie by³a sczytywana liczba klatek na sekundê FPS (ang. Frames Per Second):

\begin{enumerate}
	\item \textbf{Obraz nr 1} wykonywany by³ z widocznym znakiem STOP, a tak¿e samochodem stoj¹cym za skrzy¿owaniem. W tym przypadku podczas s³onecznego dnia o ma³ym natê¿eniu ruchu dla du¿ego modelu:
	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{Rysunki/Rozdzial4/sun10car20ppl1.png}
		\caption[sun10car20ppl1]{Miejsce wykonywania obrazu nr 1.}
		\label{fig:sun10car20ppl1}
	\end{figure} 
	
	\item \textbf{Obraz nr 2} wykonywany by³ z widocznymi motorami oraz samochodami na zakrêcie na chodniku. W tym przypadku podczas bezchmurnej nocy o œrednim natê¿eniu ruchu dla modelu du¿ego:
	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{Rysunki/Rozdzial4/night20car40ppl2.png}
		\caption[night20car40ppl2]{Miejsce wykonywania obrazu nr 2.}
		\label{fig:night20car40ppl2}
	\end{figure} 
	
	\item \textbf{Obraz nr 3} wykonywany by³ w miejscu stanowi¹cym wyzwanie dla YOLO, poniewa¿ by³o to ruchliwe skrzy¿owanie z sygnalizacj¹ œwietln¹, samochodami oraz pieszymi. Poni¿szy rysunek przedstawia scenariusz podczas deszczowego dnia o ma³ym natê¿eniu ruchu dla du¿ego modelu:
	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{Rysunki/Rozdzial4/rainsun10car20ppl3.png}
		\caption[rainsun10car20ppl3]{Miejsce wykonywania obrazu nr 3.}
		\label{fig:rainsun10car20ppl3}
	\end{figure} 
\end{enumerate}

\subsection{Œrodowisko sprzêtowe i konfiguracja}

Eksperymenty przeprowadzono na komputerze o nastêpuj¹cej konfiguracji sprzêtowej:
\begin{itemize}
	\item procesor: Intel Xeon E5-2697v2 (12 rdzeni, 24 w¹tki),
	\item pamiêæ RAM: 32 GB,
	\item dysk: 500 GB,
	\item karta graficzna: NVIDIA RTX 3060 Ti (8 GB VRAM),
	\item system operacyjny: Ubuntu 18.04.
\end{itemize}

Filmy z przejazdów nagrywano za pomoc¹ programu \texttt{OBS Studio} i zapisywano w formacie \texttt{.mkv}, natomiast zrzuty ekranu z symulatora przechowywano w formacie \texttt{.png}. Testy wykonano dla dwóch modeli detekcji: ma³ego \texttt{yolov4-tiny-416} oraz du¿ego \texttt{yolov4-416}. Dla ka¿dego scenariusza rejestrowano wartoœci \textit{FPS}, co umo¿liwi³o ocenê wydajnoœci systemu w ró¿nych warunkach symulacji.

Testy by³y przeprowadzane równie¿ dla modeli o ró¿nym stopniu skomplikowania, w tym modelu ma³ego \texttt{yolov4-tiny-416} oraz modelu du¿ego \texttt{yolov4-416}, z rejestrowaniem wartoœci \textit{FPS} w ka¿dym przypadku, co pozwoli³o na ocenê wydajnoœci systemu w ró¿nych warunkach symulacji.

\subsection{Analiza wyników wydajnoœciowych}

Na podstawie tabel \ref{tab:wyniki_model_duzy} oraz \ref{tab:wyniki_model_maly} mo¿na zauwa¿yæ, ¿e liczba klatek na sekundê po stronie serwera jest bardzo podobna dla obu modeli detekcji. W warunkach dziennych przy s³onecznej pogodzie i ma³ym natê¿eniu ruchu serwer osi¹ga typowo 20--25 FPS, zarówno dla modelu \texttt{yolov4-416}, jak i \texttt{yolov4-tiny-416}. Przy wzroœcie liczby pojazdów i pieszych do poziomu œredniego oraz du¿ego wartoœci te stopniowo spadaj¹ do oko³o 14--19 FPS, co widaæ zarówno w scenariuszach „dzieñ, s³oñce”, jak i „dzieñ, deszcz”. Oznacza to, ¿e w tych konfiguracjach g³ównym czynnikiem wp³ywaj¹cym na wydajnoœæ serwera jest z³o¿onoœæ sceny w symulatorze CARLA, a nie rozmiar zastosowanego modelu detekcji.

Najwiêksze ró¿nice miêdzy poszczególnymi scenariuszami pogodowymi i porami dnia pojawiaj¹ siê w przypadku scen nocnych oraz przy intensywnych opadach deszczu. Dla konfiguracji „noc, czyste niebo” oraz „noc, deszcz” wartoœci FPS na serwerze spadaj¹ do poziomu oko³o 6--11 FPS niezale¿nie od tego, czy wykorzystywany jest model du¿y, czy ma³y. Wynika to z wiêkszej z³o¿onoœci oœwietlenia, licznych Ÿróde³ œwiat³a sztucznego oraz dodatkowych efektów pogodowych, takich jak odbicia na mokrej nawierzchni. Symulator musi wówczas przetwarzaæ bardziej wymagaj¹c¹ scenê 3D, co ogranicza liczbê generowanych klatek i w praktyce „dominuje” koszty obliczeniowe wzglêdem samej detekcji obiektów.

WyraŸne ró¿nice miêdzy modelami widoczne s¹ natomiast po stronie klienta. Dla modelu \texttt{yolov4-416} czêœæ kliencka, odpowiedzialna za wizualizacjê i logikê sterowania, pracuje zwykle z prêdkoœci¹ oko³o 4--8 FPS we wszystkich analizowanych scenariuszach. W przypadku modelu \texttt{yolov4-tiny-416} wartoœci te rosn¹ do przedzia³u 11--16 FPS, przy czym najwy¿sze wyniki osi¹gane s¹ w dzieñ przy ma³ym natê¿eniu ruchu, a najni¿sze w nocy przy du¿ej liczbie aktorów. Pokazuje to, ¿e uproszczony model detekcji znacz¹co zmniejsza obci¹¿enie CPU oraz koszty przetwarzania danych po stronie klienta, co przek³ada siê na bardziej p³ynne dzia³anie interfejsu.

Podsumowuj¹c, wyniki eksperymentu on-line wskazuj¹, ¿e w badanej konfiguracji sprzêtowej w¹skim gard³em wydajnoœci jest g³ównie symulator CARLA uruchomiony na GPU, który ogranicza liczbê FPS na serwerze do wartoœci rzêdu 6--25 w zale¿noœci od warunków pogodowych i natê¿enia ruchu. Zastosowanie modelu \texttt{yolov4-tiny-416} poprawia natomiast odczuwaln¹ p³ynnoœæ pracy po stronie klienta, umo¿liwiaj¹c wygodniejsze testowanie systemu w czasie zbli¿onym do rzeczywistego, zw³aszcza w scenach dziennych o mniejszej z³o¿onoœci.

\begin{table}[H]
	\centering
	\adjustbox{max width=\textwidth}{  % Automatyczne dopasowanie szerokoœci tabeli do szerokoœci strony
		\renewcommand{\arraystretch}{1.3}  % Zwiêkszenie odstêpów w tabeli
		\begin{tabular}{|c|l|c|c|c|}
			\hline
			\multirow{2}{*}{\textbf{Lp.}} & \multirow{2}{*}{\textbf{Scenariusz testowy}} & \multicolumn{3}{c|}{\textbf{Obrazy [FPS]}} \\
			\cline{3-5}
			& & \textbf{Nr 1 Serwer/ Klient} & \textbf{Nr 2 Serwer/ Klient} & \textbf{Nr 3 Serwer/ Klient} \\  
			\hline
			1.  & Dzieñ, S³oñce, Ma³y ruch  &  20/11  &  25/12  &  23/11  \\
			\hline
			2.  & Dzieñ, S³oñce, Œredni ruch  &  16/11  &  22/12  &  20/13  \\
			\hline
			3.  & Dzieñ, S³oñce, Du¿y ruch  &  14/13  &  19/12  &  17/12  \\
			\hline
			4.  & Dzieñ, Deszcz, Ma³y ruch  &  19/12  &  23/11  &  22/11  \\
			\hline
			5.  & Dzieñ, Deszcz, Œredni ruch  &  16/13  &  20/11  &  19/12  \\
			\hline
			6.  & Dzieñ, Deszcz, Du¿y ruch  &  14/14  &  18/12  &  17/12  \\
			\hline
			7.  & Noc, Czyste niebo, Ma³y ruch  &  7/15  &  9/12  & 9/13  \\
			\hline
			8.  & Noc, Czyste niebo, Œredni ruch  &  6/14  &  7/13  &  8/12  \\
			\hline
			9.  & Noc, Czyste niebo, Du¿y ruch  &  6/13  &  8/14  &  8/14  \\
			\hline
			10. & Noc, Deszcz, Ma³y ruch  &  7/14  &  9/13  &  9/12  \\
			\hline
			11. & Noc, Deszcz, Œredni ruch  &  7/14  &  8/13  &  8/14  \\
			\hline
			12. & Noc, Deszcz, Du¿y ruch  &  7/15  &  8/16  &  7/15  \\
			\hline
		\end{tabular}
	}
	\caption{Wyniki testów wydajnoœciowych modelu ma³ego.}
	\label{tab:wyniki_model_maly}
\end{table}

\begin{table}[h]
	\centering
	\resizebox{\textwidth}{!}{
		\renewcommand{\arraystretch}{1.3} % Zwiêkszenie odstêpów w tabeli
		\begin{tabular}{|c|l|c|c|c|}
			\hline
			\multirow{2}{*}{\textbf{Lp.}} & \multirow{2}{*}{\textbf{Scenariusz testowy}} & \multicolumn{3}{c|}{\textbf{Obrazy  [FPS]}} \\
			\cline{3-5}
			& & \textbf{Nr 1} \textbf{Serwer/ Klient} & \textbf{Nr 2} \textbf{Serwer/ Klient} & \textbf{Nr 3} \textbf{Serwer/ Klient} \\  
			\hline
			1.  & Dzieñ, S³oñce, Ma³y ruch  &  20/ 4  &  25/ 4  &  23/ 4  \\
			\hline
			2.  & Dzieñ, S³oñce,  Œredni ruch  &  18/ 4  &  22/ 4  &  19/ 4  \\
			\hline
			3.  & Dzieñ, S³oñce,  Du¿y ruch  &  14/ 4  &  15/ 4  &  16/ 4  \\
			\hline
			4.  & Dzieñ, Deszcz, Ma³y ruch  &  19/ 4  &  22/ 4  &  21/ 4  \\
			\hline
			5.  & Dzieñ, Deszcz, Œredni ruch  &  16/ 4  &  19/ 4  &  19/ 4  \\
			\hline
			6.  & Dzieñ, Deszcz, Du¿y ruch  &  14/ 4  &  16/ 4  &  16/ 4  \\
			\hline
			7.  & Noc, Czyste niebo, Ma³y ruch  &  11/ 4  &  22/ 4  &  12/ 4  \\
			\hline
			8.  & Noc, Czyste niebo, Œredni ruch  &  6/ 4  &  8/ 4  &  8/ 4 \\
			\hline
			9.  & Noc, Czyste niebo, Du¿y ruch  &  6/ 4  &  8/ 4  &  8/ 4  \\
			\hline
			10. & Noc, Deszcz, Ma³y ruch  &  7/ 4  &  8/ 4 &  8/ 4  \\
			\hline
			11. & Noc, Deszcz, Œredni ruch  &  6/ 4  &  8/ 4  &  8/ 4  \\
			\hline
			12. & Noc, Deszcz, Du¿y ruch  &   6/ 4  &  8/ 4  &  8/ 4  \\
			\hline
		\end{tabular}
	}
	\caption{Wyniki testów wydajnoœciowych dla modelu du¿ego.}
	\label{tab:wyniki_model_duzy}
\end{table}

\subsection{Przyk³adowe funkcjonalnoœci oprogramowania w trybie on-line}

Zaimplementowane oprogramowanie w œrodowisku symulacyjnym CARLA, rozszerzone o integracjê z algorytmem detekcji YOLOv4, oferuje szereg funkcjonalnoœci pozwalaj¹cych na efektywne testowanie i wizualizacjê systemów percepcyjnych pojazdu autonomicznego. W niniejszym rozdziale przedstawiono najwa¿niejsze elementy i mechanizmy dzia³ania, które sk³adaj¹ siê na system przetwarzania i analizy danych w czasie rzeczywistym.

\begin{enumerate}
\item \textbf{Pobieranie i przetwarzanie obrazu}

Podstaw¹ dzia³ania systemu detekcji jest regularne pobieranie aktualnych danych wizualnych z renderowanej sceny symulatora. Realizowane jest to poprzez bibliotekê \texttt{pygame}, która umo¿liwia bezpoœredni dostêp do zawartoœci ekranu:

\begin{lstlisting}[style=pythonColor, emph={array3d, 0swapaxes}, caption={Pobieranie i przetwarzanie obrazu w symulatorze CARLA}]
	frame = pygame.surfarray.array3d(display)
	frame = frame.swapaxes(0,1)
\end{lstlisting}

Obraz nastêpnie poddawany jest normalizacji oraz skalowaniu do rozmiaru oczekiwanego przez sieæ YOLOv4 (domyœlnie 416x416 pikseli). W ten sposób przygotowane dane wejœciowe s¹ gotowe do przekazania do modelu detekcyjnego.

\item \textbf{Wykrywanie obiektów}

Wykorzystanie modelu YOLOv4 pozwala na detekcjê wielu klas obiektów w czasie rzeczywistym. Wczytany model (za pomoc¹ TensorFlow) analizuje dostarczony obraz i zwraca zestaw predykcji, zawieraj¹cy wspó³rzêdne obiektów oraz ich prawdopodobieñstwo klasyfikacji:

\begin{lstlisting}[style=pythonColor, emph={infer, tf.image.combined\_non\_max\_suppression}, caption={Wykrywanie obiektów za pomoc¹ YOLOv4}]
	pred_bbox = infer(batch_data)
	...
	boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(...)
\end{lstlisting}

System przetwarza surowe dane wyjœciowe, filtruj¹c i formatuj¹c wyniki z u¿yciem funkcji pomocniczych (np. \texttt{utils.format\_boxes()}). Dodatkowo odczytywane s¹ nazwy klas z pliku konfiguracyjnego, aby przypisaæ odpowiedni¹ etykietê do ka¿dego wykrytego obiektu.

\item \textbf{Filtrowanie klas}

U¿ytkownik mo¿e zdecydowaæ, które klasy obiektów maj¹ byæ uwzglêdnione podczas renderowania. Domyœlnie dozwolone s¹ wszystkie klasy zawarte w pliku \texttt{.names}, jednak istnieje mo¿liwoœæ ograniczenia listy do wybranych etykiet (np. tylko \texttt{person}, \texttt{car}):

\begin{lstlisting}[style=pythonColor, emph={allowed\_classes, class\_name}, caption={Filtrowanie wykrytych klas obiektów}]
	allowed_classes = ['person', 'car']
	...
	if class_name not in allowed_classes:
	deleted_indx.append(i)
\end{lstlisting}

Dziêki temu u¿ytkownik mo¿e ukierunkowaæ system detekcji na konkretne cele, co jest przydatne podczas testowania okreœlonych scenariuszy, np. wykrywania pieszych na przejœciach.

\item \textbf{Wizualizacja wykrytych obiektów}

Wykryte obiekty s¹ rysowane na ekranie w postaci prostok¹tów ograniczaj¹cych (ang. \textit{bounding boxes}). Ka¿dy z nich zawiera równie¿ tekstow¹ etykietê klasy obiektu:

\begin{lstlisting}[style=pythonColor, emph={pygame.draw.rect, bbox\_font.render}, caption={Wizualizacja wykrytych obiektów na ekranie}]
	pygame.draw.rect(display, (255, 255, 255), rect, 2)
	label = bbox_font.render(classname, True, (255,255,255))
\end{lstlisting}

System umo¿liwia równie¿ dynamiczne skalowanie czcionki, co wp³ywa na czytelnoœæ wyników. Takie podejœcie znacz¹co u³atwia analizê dzia³ania modelu w czasie rzeczywistym, umo¿liwiaj¹c wizualn¹ weryfikacjê dok³adnoœci wykryæ.

\item \textbf{Reakcja na dane wejœciowe}

W oprogramowaniu zaimplementowany zosta³ system obs³ugi zdarzeñ klawiatury, umo¿liwiaj¹cy rêczne sterowanie pojazdem w trybie symulacyjnym:

\begin{lstlisting}[style=pythonColor, emph={KeyboardControl, controller.parse\_events}, caption={Obs³uga zdarzeñ klawiatury w symulatorze}]
	controller = KeyboardControl(world, args.autopilot)
	if controller.parse_events(client, world, clock):
	return
\end{lstlisting}

Dziêki temu operator mo¿e aktywnie testowaæ system detekcji w ró¿nych sytuacjach – zmieniaj¹c prêdkoœæ pojazdu, tor jazdy czy ustawienia kamery.

\item \textbf{Integracja komponentów w pêtli g³ównej}

Wszystkie powy¿sze funkcje zosta³y zintegrowane w g³ównej pêtli gry \texttt{game\_loop()}, która odpowiada za nieprzerwan¹ aktualizacjê symulacji, wykrywanie obiektów i renderowanie wyników:

\begin{lstlisting}[style=pythonColor, emph={game\_loop, world.render}, caption={Pêtla g³ówna symulacji CARLA}]
	while True:
	clock.tick_busy_loop(60)
	...
	detections = []
	...
	world.render(display, detections)
	pygame.display.flip()
\end{lstlisting}

System dzia³a w czasie rzeczywistym z czêstotliwoœci¹ odœwie¿ania obrazu oko³o 60 FPS, co czyni go u¿ytecznym narzêdziem do eksperymentów w dziedzinie autonomicznej jazdy.
\end{enumerate}

\section{Eksperyment offline -- weryfikacja poprawnoœci detekcji}

Drugim etapem badañ by³ eksperyment przeprowadzony w trybie offline, którego celem by³a iloœciowa ocena poprawnoœci dzia³ania detektora YOLOv4. W odró¿nieniu od testów on-line, w których analizowano g³ównie wydajnoœæ systemu mierzon¹ liczb¹ klatek na sekundê, w eksperymencie offline porównywano wyniki detekcji sieci z danymi referencyjnymi (\textit{ground truth}) generowanymi przez symulator CARLA na podstawie trójwymiarowych bry³ \textit{bounding box} przypisanych do aktorów na scenie.

Szczegó³owy schemat procedury ewaluacji offline, obejmuj¹cy zapis anotacji z symulatora, uruchamianie detektora YOLOv4 w trybie wsadowym oraz obliczanie metryki Intersection over Union (IoU), zosta³ opisany w podrozdziale~3.6. W niniejszym rozdziale ograniczono siê do prezentacji g³ównych za³o¿eñ eksperymentu oraz wybranych rezultatów.

Do testów wybrano reprezentatywny zestaw scen obejmuj¹cych ró¿ne pory dnia (dzieñ, noc), warunki pogodowe (s³oñce, deszcz) oraz poziomy natê¿enia ruchu (ma³y, œredni, du¿y). Dla ka¿dej kombinacji wygenerowano sekwencjê klatek z kamery RGB oraz odpowiadaj¹ce im pliki JSON zawieraj¹ce anotacje \textit{ground truth}. Nastêpnie te same obrazy zosta³y przetworzone przez model YOLOv4 w trybie offline, a skrypt ewaluacyjny obliczy³ wartoœci IoU dla dopasowanych par ramek oraz zliczy³ liczby detekcji poprawnych (TP), pominiêtych (FN) i fa³szywych (FP).

Uzyskane wyniki wskazuj¹, ¿e najwy¿sze wartoœci œredniego IoU oraz najwiêkszy odsetek poprawnych detekcji osi¹gane s¹ w warunkach dziennych przy dobrym oœwietleniu i niewielkim natê¿eniu ruchu. W scenariuszach nocnych oraz przy intensywnych opadach deszczu obserwuje siê obni¿enie IoU oraz wzrost liczby b³êdnych detekcji, co jest spójne z wynikami testów wydajnoœciowych i potwierdza, ¿e trudne warunki œrodowiskowe wp³ywaj¹ zarówno na liczbê klatek na sekundê, jak i na jakoœæ predykcji modelu.

\subsection{Metryka Intersection over Union (IoU)}

Do iloœciowej oceny poprawnoœci dzia³ania detektora wykorzystano metrykê Intersection over Union (IoU), porównuj¹c¹ prostok¹ty referencyjne pochodz¹ce z symulatora CARLA z ramkami przewidywanymi przez sieæ YOLOv4. Dla ka¿dej pary ramek tej samej klasy oblicza siê stosunek pola czêœci wspólnej do pola sumy obu prostok¹tów, zgodnie z zale¿noœci¹
\[
\mathrm{IoU} = \frac{\mathrm{area}(B_{\text{CARLA}} \cap B_{\text{YOLO}})}{\mathrm{area}(B_{\text{CARLA}} \cup B_{\text{YOLO}})}.
\]
Wartoœæ IoU zawiera siê w przedziale od 0 do 1, gdzie 1 oznacza idealne pokrycie obiektu przez predykcjê modelu, natomiast wartoœci bliskie 0 œwiadcz¹ o du¿ym przesuniêciu lub znacznym niedopasowaniu rozmiarów ramek. W eksperymencie przyjêto, ¿e detekcja jest poprawna, je¿eli klasy obiektu s¹ zgodne, a IoU przekracza ustalony próg (np. 0,5), co pozwala na zliczanie liczby trafieñ (TP), pominiêæ (FN) oraz fa³szywych detekcji (FP) w analizowanych scenach.

\subsection{Zestaw scen i konfiguracja eksperymentu offline}

Eksperyment offline zosta³ przeprowadzony na serii przejazdów zarejestrowanych
w symulatorze CARLA, ró¿ni¹cych siê konfiguracj¹ œrodowiska, warunkami
atmosferycznymi oraz z³o¿onoœci¹ sceny. Dla ka¿dego przejazdu zapisano
sekwencjê klatek z kamery RGB skierowanej do przodu pojazdu oraz odpowiadaj¹ce
im pliki JSON z anotacjami \textit{ground truth}, generowanymi przez skrypt
\texttt{bounding\_boxes.py}. Anotacje zawiera³y informacje o po³o¿eniu oraz
klasie obiektów widocznych w obrazie, co umo¿liwia³o póŸniejsze porównanie
dzia³ania algorytmu detekcji z danymi referencyjnymi.

W trakcie generowania anotacji uwzglêdniano wy³¹cznie obiekty znajduj¹ce siê
w odleg³oœci do 75 metrów od kamery (parametr zasiêgu detekcji oznaczony jako
\texttt{d75}). Ograniczenie to wynika z typowego zasiêgu skutecznej detekcji
dla kamer stosowanych w pojazdach oraz z faktu, ¿e obiekty po³o¿one dalej maj¹
znikomy wp³yw na bie¿¹ce decyzje uk³adów wspomagania kierowcy. Dodatkowo
pozwala to unikn¹æ bardzo ma³ych obiektów w dalszym planie, których poprawne
oznaczenie w anotacjach oraz detekcja przez model s¹ szczególnie trudne.

Przejazdy realizowano na dwóch mapach dostêpnych w symulatorze, oznaczonych
jako \textit{Town03} oraz \textit{Town04}. Pierwsza z nich reprezentuje wiêkszy
obszar miejski z wieloma skrzy¿owaniami, rondem oraz rozbudowan¹ infrastruktur¹
drogow¹, natomiast druga odwzorowuje fragment drogi szybkiego ruchu poprowadzonej
poza zabudow¹ miejsk¹, z charakterystycznym uk³adem w kszta³cie „ósemki”
oraz otoczeniem leœnym.[web:77] W obu przypadkach scena by³a uzupe³niona o ruch
pojazdów sterowanych przez wbudowany system ruchu drogowego, co pozwoli³o
uzyskaæ zró¿nicowane rozk³ady po³o¿eñ i prêdkoœci obiektów.

Docelowa liczba aktorów w scenie zosta³a ustawiona na oko³o 200. Wiêkszoœæ z nich
stanowi³y pojazdy (\textit{vehicles}), jednak w otoczeniu mog³y pojawiaæ siê
równie¿ inni uczestnicy ruchu, tacy jak piesi czy rowerzyœci. Nale¿y podkreœliæ,
¿e liczba obiektów obecnych w pojedynczej klatce nie jest dok³adnie równa 200,
gdy¿ symulator dynamicznie do³¹cza i usuwa aktorów w zale¿noœci od ich po³o¿enia
wzglêdem aktualnie aktywnego obszaru mapy oraz bie¿¹cych warunków ruchu.
W praktyce przek³ada siê to na zmienn¹ gêstoœæ ruchu, co jest korzystne z punktu
widzenia testowania algorytmu w ró¿nych scenariuszach – od wzglêdnie pustej
drogi po sytuacje bardziej zat³oczone.

Dla ka¿dej mapy przygotowano kilka wariantów pogodowych i oœwietleniowych,
odpowiadaj¹cych predefiniowanym ustawieniom symulatora, takim jak s³oneczny dzieñ
(\textit{ClearNoon}), deszcz w ci¹gu dnia (\textit{HardRainNoon}) czy noc bez
zachmurzenia (\textit{ClearNight}).[web:82] Pozwoli³o to oceniæ, jak na jakoœæ
detekcji wp³ywaj¹ warunki oœwietleniowe (równomierne oœwietlenie w po³udnie,
silne kontrasty podczas zachodu s³oñca, ograniczona widocznoœæ i punktowe
Ÿród³a œwiat³a w nocy) oraz efekty pogodowe, takie jak intensywne opady deszczu
i mokra nawierzchnia. W scenach deszczowych dodatkowym utrudnieniem s¹ krople
na obiektywie kamery oraz odbicia œwiate³ na jezdni, które mog¹ byæ mylone przez
model z rzeczywistymi obiektami.

Na mapie \textit{Town03} rejestrowano g³ównie sceny miejskie, obejmuj¹ce szersze
skrzy¿owania, zabudowê oraz elementy infrastruktury drogowej, takie jak sygnalizacja
œwietlna czy przejœcia dla pieszych.[web:77] W scenariuszu dziennym, przy dobrej
pogodzie, ulice s¹ jasno oœwietlone, a w kadrze pojawiaj¹ siê pojedyncze pojazdy
oraz motocykle, co sprzyja wyraŸnemu odwzorowaniu konturów obiektów i oznaczeñ
poziomych. W wariancie deszczowej nocy obraz staje siê znacznie trudniejszy do
analizy: oœwietlenie pochodzi g³ównie z lamp ulicznych i reflektorów pojazdów,
a mokra nawierzchnia powoduje liczne odbicia œwiat³a. W po³¹czeniu z ciemnym
t³em zabudowy prowadzi to do powstawania obszarów przeœwietlonych oraz fragmentów
sceny pogr¹¿onych w cieniu, co stanowi istotne utrudnienie dla algorytmu detekcji.

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial4/town_03_cars_200_clear_noon_d75_scen_01.png}
	\caption{Scenariusz \textit{Town03, dzieñ, s³oñce} z widocznymi pojazdami na skrzy¿owaniu.}
	\label{fig:town03_clear_noon_offline}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial4/town_03_cars_200_rainy_night_d75_scen_01.png}
	\caption{Scenariusz \textit{Town03, deszczowa noc} z odbiciami œwiate³ na mokrej nawierzchni.}
	\label{fig:town03_rainy_night_offline}
\end{figure}

Mapa \textit{Town04} odwzorowuje odcinek drogi szybkiego ruchu w terenie
poza miejskim, z otoczeniem w postaci lasu oraz zbiorników wodnych.[web:79]
Dominuj¹ tu d³u¿sze, proste odcinki oraz ³agodne ³uki, a ruch odbywa siê z
wiêkszymi prêdkoœciami ni¿ w przypadku scen miejskich. W scenariuszu dziennym
z intensywnymi opadami deszczu widocznoœæ w dalszej czêœci sceny ograniczaj¹
krople deszczu oraz zamglenie, co wp³ywa na kontrast miêdzy pojazdami a t³em.
Mokra nawierzchnia drogi powoduje dodatkowo powstawanie odbiæ, które mog¹
zaburzaæ wizualn¹ separacjê pasów ruchu oraz sylwetek pojazdów.

W scenariuszu nocnym przy bezchmurnym niebie dominuj¹ punktowe Ÿród³a œwiat³a,
takie jak latarnie i reflektory pojazdów, co prowadzi do du¿ych ró¿nic jasnoœci
pomiêdzy poszczególnymi fragmentami obrazu. Czêœæ obiektów znajduje siê w
pó³mroku, a czêœæ jest silnie oœwietlona, co mo¿e skutkowaæ utrat¹ detali w
jasnych obszarach oraz brakiem informacji w cieniach. Z punktu widzenia
eksperymentu offline pozwala to jednak zweryfikowaæ, w jakim stopniu model
detekcji radzi sobie z takimi skrajnymi warunkami i czy zachowuje stabilnoœæ
wyników w otoczeniu drogi szybkiego ruchu.

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial4/town_04_cars_200_hard_rain_noon_d75_scen_01.png}
	\caption{Scenariusz \textit{Town04, dzieñ, silny deszcz} na drodze szybkiego ruchu.}
	\label{fig:town04_hard_rain_noon_offline}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial4/town_04_cars_200_clear_night_d75_scen_01.png}
	\caption{Scenariusz \textit{Town04, noc, czyste niebo} z punktowymi Ÿród³ami œwiat³a.}
	\label{fig:town04_clear_night_offline}
\end{figure}

\subsection{Przebieg przetwarzania w eksperymencie offline}

Ca³y eksperyment offline zosta³ zrealizowany jako sekwencja kroków, w której ka¿dy etap odpowiada osobnemu skryptowi. Pozwoli³o to oddzieliæ proces generowania danych referencyjnych od detekcji YOLOv4 oraz od w³aœciwej ewaluacji.

W pierwszym etapie uruchamiany jest skrypt \texttt{bounding\_boxes.py}, który podczas przejazdu w symulatorze CARLA odczytuje listê aktorów znajduj¹cych siê w scenie oraz ich struktury \texttt{carla.BoundingBox}. Na tej podstawie wyznaczane s¹ obrysy 2D obiektów w obrazie z kamery, ograniczone do zasiêgu 75 metrów od punktu obserwacji. Dla ka¿dej klatki generowany jest plik JSON zawieraj¹cy anotacje \textit{ground truth}: identyfikator obrazu, klasê obiektu oraz wspó³rzêdne prostok¹ta w pikselach.

W drugim etapie wykorzystywany jest skrypt \texttt{yolo\_cpu.py}, który w trybie wsadowym wczytuje zapisane wczeœniej obrazy i przetwarza je za pomoc¹ modelu YOLOv4. Dla ka¿dej klatki zapisywana jest lista wykrytych obiektów z podan¹ klas¹, ramk¹ 2D oraz wspó³czynnikiem pewnoœci detekcji. Wyniki te trafiaj¹ do osobnych plików JSON, co umo¿liwia ich póŸniejsz¹ analizê niezale¿nie od dzia³ania symulatora.

Trzeci etap stanowi w³aœciwa ewaluacja jakoœci detekcji, realizowana przez skrypt \texttt{evaluate\_iou.py}. Program ten wczytuje pary plików JSON z anotacjami CARLA oraz z predykcjami YOLOv4, dopasowuje ramki tej samej klasy na podstawie maksymalnej wartoœci IoU powy¿ej ustalonego progu i oblicza statystyki jakoœciowe. Dla ka¿dego przejazdu wyznaczane s¹ m.in. œrednie wartoœci IoU, liczba trafieñ (TP), fa³szywych detekcji (FP) oraz pominiêæ obiektów (FN).

Dodatkowo, pomocniczy skrypt \texttt{bbox\_image.py} umo¿liwia wizualn¹ weryfikacjê wyników. Na podstawie wybranych plików JSON rysuje on ramki pochodz¹ce z CARLA lub z YOLOv4 na zapisanych obrazach, co pozwala na ³atwe wyszukanie klatek z bardzo dobrym, przeciêtnym oraz s³abym dopasowaniem i zilustrowanie ich w dalszej czêœci rozdzia³u.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{
		\renewcommand{\arraystretch}{1.3}
		\begin{tabular}{|l|c|c|}
			\hline
			\textbf{Scenariusz} & \textbf{Liczba klatek} & \textbf{Œrednie IoU} \\
			\hline
			Town03, dzieñ, s³oñce, aktorzy $\approx$ 200       & 378 & 0{,}76 \\
			\hline
			Town03, noc, czyste niebo, aktorzy $\approx$ 200   & 333 & 0{,}69 \\
			\hline
			Town03, zachód s³oñca, aktorzy $\approx$ 200       & 223 & 0{,}72 \\
			\hline
			Town03, zachód s³oñca, aktorzy $\approx$ 200       & 445 & 0{,}71 \\
			\hline
			Town03, dzieñ, silny deszcz, aktorzy $\approx$ 200 & 400 & 0{,}64 \\
			\hline
			Town03, deszczowa noc, aktorzy $\approx$ 200       & 429 & 0{,}58 \\
			\hline
			Town04, noc, czyste niebo, aktorzy $\approx$ 200   & 262 & 0{,}68 \\
			\hline
			Town04, dzieñ, s³oñce, aktorzy $\approx$ 200       & 326 & 0{,}75 \\
			\hline
			Town04, dzieñ, s³oñce, aktorzy $\approx$ 200       & 302 & 0{,}74 \\
			\hline
			Town04, zachód s³oñca, aktorzy $\approx$ 200       & 336 & 0{,}70 \\
			\hline
			Town04, dzieñ, silny deszcz, aktorzy $\approx$ 200 & 286 & 0{,}63 \\
			\hline
			Town04, deszczowa noc, aktorzy $\approx$ 200       & 408 & 0{,}56 \\
			\hline
		\end{tabular}
	}
	\caption{Przyk³adowe wyniki eksperymentu offline dla wszystkich zarejestrowanych scen symulacji CARLA.}
	\label{tab:iou_scenarios}
\end{table}

\subsection{Analiza wyników eksperymentu offline}

Zestawienie wyników w tab.~\ref{tab:iou_scenarios} pokazuje, ¿e wartoœci œredniego IoU silnie zale¿¹ od warunków oœwietleniowych i pogodowych. Poni¿ej przedstawiono przyk³adowe klatki z na³o¿onymi ramkami \textit{ground truth} pochodz¹cymi z symulatora CARLA oraz detekcjami sieci YOLO dla dwóch scenariuszy, w których model radzi sobie wyraŸnie s³abiej ni¿ w s³oneczny dzieñ.

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial4/town03_clear_night_carla.png}
	\caption{Scenariusz \textit{Town03, noc, czyste niebo} -- ramki \textit{ground truth} CARLA dla pojazdów widocznych w kadrze.}
	\label{fig:town03_clear_night_carla}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial4/town03_clear_night_yolo.png}
	\caption{Scenariusz \textit{Town03, noc, czyste niebo} -- detekcje YOLO dla tej samej klatki.}
	\label{fig:town03_clear_night_yolo}
\end{figure}

W scenariuszu \textit{Town03, clear night} widoczna jest wyraŸna ró¿nica pomiêdzy liczb¹ obiektów oznaczonych w anotacjach CARLA a liczb¹ ramek wygenerowanych przez YOLO. Czêœæ pojazdów pozostaje niewykryta, poniewa¿ ich sylwetki s¹ zbyt ciemne lub znajduj¹ siê w znacznej odleg³oœci od kamery, przez co zajmuj¹ jedynie kilka pikseli w obrazie. Powoduje to spadek œredniego IoU oraz wzrost liczby pominiêæ (FN), mimo relatywnie niewielkiej liczby fa³szywych detekcji.

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial4/town03_hard_rain_noon_carla.png}
	\caption{Scenariusz \textit{Town03, dzieñ, silny deszcz} -- ramki \textit{ground truth} CARLA na autostradzie.}
	\label{fig:town03_hard_rain_noon_carla}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial4/town03_hard_rain_noon_yolo.png}
	\caption{Scenariusz \textit{Town03, dzieñ, silny deszcz} -- detekcje YOLO dla tej samej klatki.}
	\label{fig:town03_hard_rain_noon_yolo}
\end{figure}

Drugi przyk³ad pochodzi z przejazdu po odcinku drogi szybkiego ruchu w warunkach \textit{Town03, hard rain noon}. Pomimo dziennej pory i lepszego oœwietlenia ni¿ w scenie nocnej, intensywne opady deszczu obni¿aj¹ kontrast pomiêdzy pojazdami a t³em oraz czêœciowo rozmywaj¹ ich kontury. YOLO nadal nie odtwarza wszystkich obiektów z anotacji CARLA, choæ wykrywa ich wiêcej ni¿ w scenie nocnej – szczególnie dobrze rozpoznawane s¹ pojazdy znajduj¹ce siê bli¿ej kamery, natomiast pominiêcia dotycz¹ g³ównie obiektów dalszych oraz czêœciowo zas³oniêtych.

\paragraph{Podsumowanie.}
Przedstawione przyk³ady potwierdzaj¹, ¿e najwiêksze problemy detektor napotyka w sytuacjach, gdy informacja wizualna o obiekcie jest ograniczona przez s³abe oœwietlenie, intensywne opady lub du¿¹ odleg³oœæ od kamery. Z punktu widzenia dalszego rozwoju systemu oznacza to potrzebê rozszerzenia zbioru treningowego o sceny nocne i deszczowe oraz rozwa¿enia zastosowania metod poprawy kontrastu i redukcji szumu w obrazie przed uruchomieniem detektora.

\subsection{Wybrane funkcjonalnoœci oprogramowania w eksperymencie offline}

Do realizacji eksperymentu offline wykorzystano kilka skryptów pomocniczych, odpowiedzialnych za kolejne etapy przygotowania danych, detekcji oraz ewaluacji. Poni¿ej przedstawiono najwa¿niejsze z nich wraz z przyk³adowymi fragmentami kodu.

\subsubsection{Skrypt \texttt{bounding\_boxes.py} -- generowanie anotacji referencyjnych}

Skrypt \texttt{bounding\_boxes.py}, dostarczony przez twórców symulatora CARLA, odpowiada za generowanie anotacji \textit{ground truth} w postaci ramek ograniczaj¹cych w przestrzeni 3D i 2D. Dla ka¿dego aktora obecnego w scenie wyznaczany jest obrys bry³y 3D, rzutowany nastêpnie na p³aszczyznê obrazu kamery i zapisywany w formacie JSON.

\begin{lstlisting}[style=pythonColor,caption={Dodawanie obiektu do anotacji w \texttt{bounding\_boxes.py}},label={lst:bb_append}]
	json_frame_data['objects'].append({
		'id': npc.id,
		'class': SEMANTIC_MAP[npc.semantic_tags[0]][0],
		'blueprint_id': npc.type_id,
		'velocity': calculate_relative_velocity(npc, ego_vehicle),
		'bbox_3d': npc_bbox_3d['bbox_3d'],
		'bbox_2d': {
			'xmin': int(npc_bbox_2d['bbox_2d'][0]),
			'ymin': int(npc_bbox_2d['bbox_2d'][1]),
			'xmax': int(npc_bbox_2d['bbox_2d'][2]),
			'ymax': int(npc_bbox_2d['bbox_2d'][3]),
		} if npc_bbox_2d else None,
		'light_state': vehicle_light_state_to_dict(npc)
	})
\end{lstlisting}

Ka¿dy wpis w strukturze \texttt{json\_frame\_data['objects']} zawiera identyfikator aktora, jego klasê semantyczn¹, prêdkoœæ wzglêdn¹ wzglêdem pojazdu ego, parametry bry³y 3D oraz wspó³rzêdne prostok¹ta 2D opisuj¹cego po³o¿enie obiektu w obrazie. Tak przygotowane anotacje stanowi¹ punkt odniesienia w eksperymencie offline, s³u¿¹c do porównania z detekcjami uzyskanymi z modelu YOLOv4.

\subsubsection{Skrypt \texttt{yolo.py} -- detekcja YOLO i zapis JSON}

Skrypt \texttt{yolo.py} realizuje detekcjê obiektów na zapisanych wczeœniej obrazach z kamery, korzystaj¹c z biblioteki \texttt{ultralytics} i modelu YOLO. Wyniki detekcji zapisywane s¹ w plikach JSON w strukturze zbli¿onej do formatu stosowanego przez \texttt{bounding\_boxes.py}, co u³atwia póŸniejsz¹ ewaluacjê.

\begin{lstlisting}[style=pythonColor,caption={Struktura JSON z wynikami YOLO},label={lst:yolo_append}]
	for box in results.boxes:
	cls_id = int(box.cls[0])
	cls_name = model.names[cls_id]
	carla_class = yolo_to_carla_class(cls_name)
	xmin, ymin, xmax, ymax = box.xyxy[0].tolist()
	
	json_out["objects"].append({
		"id": obj_id,
		"class": carla_class,
		"blueprint_id": "yolo.detected",
		"velocity": {"x": 0, "y": 0, "z": 0},
		"bbox_3d": None,
		"bbox_2d": {
			"xmin": int(xmin),
			"ymin": int(ymin),
			"xmax": int(xmax),
			"ymax": int(ymax)
		},
		"light_state": {}
	})
\end{lstlisting}

Dla ka¿dego wykrytego obiektu zapisywana jest klasa przemapowana na odpowiadaj¹c¹ jej klasê w CARLA, prostok¹t 2D w uk³adzie pikselowym oraz pomocnicze pola, takie jak identyfikator obiektu i informacje o stanie œwiate³. Ujednolicenie formatu danych umo¿liwia bezpoœrednie zestawienie anotacji CARLA z detekcjami YOLO w kolejnym etapie eksperymentu.

\subsubsection{Skrypt \texttt{evaluate\_iou.py} -- obliczanie metryki IoU}

Skrypt \texttt{evaluate\_iou.py} odpowiada za iloœciow¹ ocenê jakoœci detekcji poprzez obliczenie wartoœci IoU miêdzy ramkami \textit{ground truth} a ramkami przewidywanymi przez YOLOv4. Podstaw¹ jest funkcja wyznaczaj¹ca Intersection over Union dla dwóch prostok¹tów 2D.

\begin{lstlisting}[style=pythonColor,caption={Obliczanie IoU dla dwóch ramek 2D},label={lst:iou_func}]
	def iou(boxA, boxB):
	xA = max(boxA["xmin"], boxB["xmin"])
	yA = max(boxA["ymin"], boxB["ymin"])
	xB = min(boxA["xmax"], boxB["xmax"])
	yB = min(boxA["ymax"], boxB["ymax"])
	
	inter = max(0, xB - xA) * max(0, yB - yA)
	areaA = (boxA["xmax"]-boxA["xmin"]) * (boxA["ymax"]-boxA["ymin"])
	areaB = (boxB["xmax"]-boxB["xmin"]) * (boxB["ymax"]-boxB["ymin"])
	union = areaA + areaB - inter
	return inter / union if union > 0 else 0.0
\end{lstlisting}

Na podstawie tej funkcji skrypt dopasowuje ramki YOLO do ramek referencyjnych, wyznacza œrednie IoU dla ka¿dej klatki oraz globaln¹ œredni¹ IoU dla ca³ego przejazdu, a nastêpnie zapisuje wyniki do pliku CSV wykorzystywanego w analizie eksperymentu offline. Pozwala to na obiektywn¹ ocenê dok³adnoœci lokalizacji obiektów przy ró¿nych scenariuszach pogodowych i natê¿eniu ruchu.

\subsubsection{Skrypt \texttt{bbox\_image.py} -- wizualizacja anotacji na obrazach}

Skrypt \texttt{bbox\_image.py} pe³ni rolê narzêdzia wizualizacyjnego, umo¿liwiaj¹cego na³o¿enie ramek ograniczaj¹cych zapisanych w plikach JSON na odpowiadaj¹ce im obrazy. U³atwia to rêczn¹ inspekcjê jakoœci anotacji oraz tworzenie ilustracji przedstawiaj¹cych przyk³ady dobrych, œrednich i s³abych dopasowañ ramek.

\begin{lstlisting}[style=pythonColor,caption={Rysowanie ramek na obrazie na podstawie pliku JSON},label={lst:bbox_image_short}]
	for obj in objects:
	bbox = obj.get("bbox_2d")
	if bbox is None:
	continue
	
	xmin = int(bbox["xmin"]); ymin = int(bbox["ymin"])
	xmax = int(bbox["xmax"]); ymax = int(bbox["ymax"])
	label = obj.get("class", "object")
	
	cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)
	cv2.putText(img, label, (xmin, ymin - 5),
	cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
\end{lstlisting}

Dla ka¿dego obiektu odczytywanego z pliku JSON skrypt rysuje prostok¹t 2D oraz podpis z nazw¹ klasy, a wynikowy obraz zapisywany jest do osobnego katalogu. Tak przygotowane wizualizacje wykorzystano w pracy do zilustrowania jakoœci detekcji w wybranych scenach eksperymentu offline.
