\chapter{System rozpoznawania obrazÛw}

\textbf{Rozpoznawanie obrazÛw} to dziedzina komputerowego przetwarzania danych, ktÛrej celem jest identyfikacja i klasyfikacja obiektÛw w obrazach cyfrowych. W ostatnich latach, dziÍki rozwojowi g≥Íbokiego uczenia (deep learning) oraz sieci neuronowych, osiπgniÍto znaczπcy postÍp w tej dziedzinie.

\textbf{Sieci neuronowe} to modele matematyczne inspirowane strukturπ ludzkiego mÛzgu, sk≥adajπce siÍ z po≥πczonych ze sobπ neuronÛw (wÍz≥Ûw), ktÛre przetwarzajπ informacje. G≥Íbokie uczenie odnosi siÍ do sieci neuronowych o wielu warstwach (tzw. g≥Íbokich sieci), ktÛre potrafiπ uczyÊ siÍ reprezentacji danych na rÛønych poziomach abstrakcji. W kontekúcie rozpoznawania obrazÛw, najczÍúciej stosuje siÍ konwolucyjne sieci neuronowe (Convolutional Neural Networks, CNN), ktÛre sπ szczegÛlnie efektywne w analizie danych obrazowych.

Poniøej przedstawiono \textbf{przeglπd kluczowych architektur} stosowanych w rozpoznawaniu obrazÛw, wraz z ich charakterystykπ i przyk≥adami.

\begin{figure}[htbp]
	\centering
	\vspace{-0.75\baselineskip}
	\includegraphics[width=12cm]{Rysunki/Rozdzial3/RCNN.png}
	\caption[RCNN]{Schemat dzia≥ania R-CNN.}
	\label{fig:RCNN}
\end{figure}

\begin{itemize}
	\item \textbf{R-CNN (Region-based Convolutional Neural Networks)} to jedna z pierwszych skutecznych metod detekcji obiektÛw, oparta na analizie wybranych regionÛw obrazu. W pierwszym etapie stosowany jest algorytm \textit{Selective Search}, ktÛry generuje oko≥o 2000 propozycji obszarÛw potencjalnie zawierajπcych obiekty, uwzglÍdniajπc cechy takie jak kolor, tekstura czy kszta≥t. Kaødy region jest nastÍpnie skalowany i przetwarzany przez sieÊ konwolucyjnπ (CNN), co pozwala na ekstrakcjÍ cech wizualnych. Ostatecznie, klasyfikator SVM decyduje o przynaleønoúci danego regionu do konkretnej klasy, a regresor liniowy doprecyzowuje wspÛ≥rzÍdne ramki ograniczajπcej.~\cite{ren2015faster}
	
	ChoÊ metoda ta cechuje siÍ wysokπ precyzjπ detekcji, jej g≥Ûwnπ wadπ pozostaje duøe zapotrzebowanie obliczeniowe zwiπzane z koniecznoúciπ analizowania tysiÍcy regionÛw dla kaødego obrazu.
	
	\item \textbf{Fast R-CNN} to ulepszona wersja modelu R-CNN, zaprojektowana z myúlπ o zwiÍkszeniu wydajnoúci i redukcji czasoch≥onnoúci procesu detekcji obiektÛw. G≥Ûwne usprawnienie polega na zastosowaniu jednej sieci konwolucyjnej (CNN) do ekstrakcji cech z ca≥ego obrazu przed wygenerowaniem propozycji regionÛw, co eliminuje koniecznoúÊ wielokrotnego przetwarzania tych samych obszarÛw.
	
	Zamiast klasyfikatora SVM, Fast R-CNN wykorzystuje wbudowanπ warstwÍ softmax do rozpoznawania klas obiektÛw oraz regresjÍ do dok≥adnej lokalizacji ramek ograniczajπcych. Regiony zainteresowania (RoI) sπ przekszta≥cane za pomocπ operacji \textit{RoI Pooling}, ktÛra dostosowuje ich wymiary do jednolitego formatu wejúciowego dla dalszej klasyfikacji.
	
	DziÍki integracji tych rozwiπzaÒ Fast R-CNN znacznie skraca czas przetwarzania i umoøliwia efektywne wykrywanie obiektÛw przy zachowaniu wysokiej dok≥adnoúci.~\cite{girshick2015fast}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{Rysunki/Rozdzial3/fastRCNN.png}
		\caption[fastRCNN]{Schemat dzia≥ania Fast R-CNN. èrÛd≥o: \cite{stutzFastRCNN}.}
		\label{fig:fastRcnn}
	\end{figure}
	
	\item \textbf{Faster R-CNN} to zaawansowana architektura detekcji obiektÛw, ktÛra integruje SieÊ Generujπcπ Propozycje RegionÛw (Region Proposal Network, RPN) z g≥Ûwnπ sieciπ konwolucyjnπ. Kluczowπ innowacjπ jest eliminacja potrzeby korzystania z zewnÍtrznych algorytmÛw generujπcych propozycje regionÛw, co znacznie skraca czas detekcji i zwiÍksza efektywnoúÊ~\cite{ren2015faster}.
	
	Ca≥y obraz jest najpierw przetwarzany przez CNN w celu uzyskania map cech. NastÍpnie RPN generuje regiony zainteresowania bezpoúrednio na podstawie tych map, przewidujπc zarÛwno ich pozycje, jak i prawdopodobieÒstwo zawierania obiektÛw. Regiony te sπ klasyfikowane oraz doprecyzowywane przez koÒcowy modu≥ sieci. DziÍki wspÛ≥dzieleniu warstw konwolucyjnych pomiÍdzy RPN a modu≥em klasyfikacyjnym, Faster R-CNN oferuje wysokπ dok≥adnoúÊ przy znacznie lepszej wydajnoúci niø jego poprzednicy.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{Rysunki/Rozdzial3/fasterRCNN2.png}
		\caption[fasterRCNN2]{Schemat dzia≥ania Faster R-CNN.}
		\label{fig:fasterRcnn2}
	\end{figure}

	\item \textbf{SSD (Single Shot MultiBox Detector)} to jednoetapowa metoda detekcji obiektÛw, ktÛra, w przeciwieÒstwie do modelÛw opartych na regionach, takich jak R-CNN, wykonuje detekcjÍ i klasyfikacjÍ obiektÛw w jednym kroku. DziÍki temu SSD jest szybsza i bardziej efektywna, co czyni jπ odpowiedniπ do zastosowaÒ wymagajπcych przetwarzania w czasie rzeczywistym.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{Rysunki/Rozdzial3/ssd_architecture.png}
		\label{fig:ssd_architecture}
		\caption[SSD]{Schemat architektury SSD \cite{liu2016ssd}.}
	\end{figure} 
	
	SSD wykorzystuje konwolucyjne sieci neuronowe (CNN) do ekstrakcji cech obrazu i jednoczesnej predykcji po≥oøenia oraz klasy obiektÛw na wielu skalach. Model sk≥ada siÍ z:
	\begin{itemize}
		\item \textbf{Sieci bazowej} ñ najczÍúciej stosuje siÍ architekturÍ VGG16 bez w pe≥ni po≥πczonych warstw,
		\item \textbf{Dodatkowych warstw konwolucyjnych} ñ umoøliwiajπ one detekcjÍ obiektÛw na rÛønych poziomach szczegÛ≥owoúci,
		\item \textbf{Mechanizmu MultiBox} ñ generuje wiele ramki ograniczajπcych (bounding boxes) o rÛønych proporcjach i rozmiarach,
		\item \textbf{Funkcji straty} ñ sk≥ada siÍ z dwÛch komponentÛw: b≥Ídu klasyfikacji (cross-entropy loss) oraz b≥Ídu lokalizacji (smooth L1 loss).
	\end{itemize}
	
	DziÍki swojej szybkoúci i efektywnoúci SSD znajduje zastosowanie w wielu dziedzinach, takich jak:
	\begin{itemize}
		\item Systemy monitoringu wizyjnego,
		\item Rozpoznawanie obiektÛw w autonomicznych pojazdach,
		\item Aplikacje rzeczywistoúci rozszerzonej (AR),
		\item Systemy wspomagania kierowcy (ADAS).
	\end{itemize}
	
	\item \textbf{YOLO (You Only Look Once)} to kolejna jednokrokowa architektura, ktÛra traktuje detekcjÍ obiektÛw jako problem regresji, przewidujπc bezpoúrednio klasy i po≥oøenie obiektÛw w obrazie. DziÍki temu YOLO osiπga bardzo wysokπ szybkoúÊ detekcji, co jest istotne w aplikacjach wymagajπcych przetwarzania w czasie rzeczywistym.
	
	\item \textbf{DETR (Detection Transformer)} to nowoczesna architektura detekcji obiektÛw oparta na transformerach, ktÛra integruje mechanizmy uwagi (attention mechanisms) w procesie detekcji. DETR eliminuje potrzebÍ stosowania tradycyjnych metod generowania propozycji regionÛw, co upraszcza proces detekcji i pozwala na bardziej efektywne wykorzystanie danych.
\end{itemize}

Poniøsza tabela przedstawia \textbf{porÛwnanie omÛwionych architektur} pod wzglÍdem szybkoúci i dok≥adnoúci detekcji:

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|l|l|l|l|}
			\hline
			Architektura & SzybkoúÊ (FPS)              & Dok≥adnoúÊ (mAP) & Zastosowanie                         \\ 
			\hline
			R-CNN        &  1 FPS                      & Wysoka           & Analiza offline                      \\ 
			\hline
			Fast R-CNN   & \textasciitilde{}2-3 FPS    & Wysoka           & Analiza offline                      \\ 
			\hline
			Faster R-CNN & \textasciitilde{}5-10 FPS   & Bardzo wysoka    & Zastosowania wymagajπce dok≥adnoúci  \\ 
			\hline
			SSD          & \textasciitilde{}20-60 FPS  & årednia          & Zastosowania w czasie rzeczywistym   \\ 
			\hline
			YOLO         & \textasciitilde{}45-150 FPS & Wysoka           & Wykrywanie w czasie rzeczywistym     \\ 
			\hline
			DETR         & \textasciitilde{}10-20 FPS  & Bardzo wysoka    & Nowoczesne zastosowania AI           \\
			\hline
		\end{tabular}
	}
	\caption{PorÛwnanie wybranych architektur}
\end{table}

Rozpoznawanie obrazÛw z wykorzystaniem sieci neuronowych jest obecnie jednπ z kluczowych technologii w dziedzinie sztucznej inteligencji. DziÍki zastosowaniu zaawansowanych architektur, takich jak R-CNN, SSD, YOLO czy DETR, moøliwe jest osiπganie wysokiej dok≥adnoúci i szybkoúci detekcji obiektÛw. WybÛr odpowiedniej architektury zaleøy od specyficznych wymagaÒ aplikacji, takich jak potrzeba przetwarzania w czasie rzeczywistym, dostÍpnoúÊ zasobÛw obliczeniowych oraz dok≥adnoúÊ rozpoznawania.

Przysz≥y rozwÛj w tej dziedzinie prawdopodobnie bÍdzie koncentrowa≥ siÍ na dalszym zwiÍkszaniu efektywnoúci modeli, integracji z systemami opartymi na transformerach oraz rozwijaniu metod rozpoznawania obiektÛw w trudnych warunkach úrodowiskowych.
\section{YOLOv4 i konwolucyjne sieci neuronowe}

YOLOv4 (\textit{You Only Look Once version 4}) to jeden z najnowszych modeli s≥uøπcych do detekcji obiektÛw w obrazach i wideo, naleøπcy do rodziny algorytmÛw YOLO. Jego celem jest osiπgniÍcie wysokiej precyzji przy jednoczesnym zachowaniu duøej szybkoúci dzia≥ania. YOLOv4 opiera siÍ na konwolucyjnych sieciach neuronowych (CNN), ktÛre umoøliwiajπ ekstrakcjÍ cech wizualnych z obrazÛw oraz ich klasyfikacjÍ \cite{yolov4_paper}.

\subsection{Konwolucyjne sieci neuronowe (CNN)}

Konwolucyjne sieci neuronowe (CNN) sπ rodzajem sztucznych sieci neuronowych szczegÛlnie skutecznym w analizie obrazÛw. Dzia≥ajπ one w sposÛb hierarchiczny, w ktÛrym rÛøne warstwy sieci uczπ siÍ wykrywaÊ rÛøne cechy obrazu ó od prostych krawÍdzi i tekstur w poczπtkowych warstwach, po bardziej z≥oøone struktury, takie jak twarze czy obiekty w g≥Íbszych warstwach. 

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial3/cnn_structure.png}
	\caption{Schemat struktury konwolucyjnej sieci neuronowej~\cite{esezam}.}
	\label{fig:cnn}
\end{figure}

Konwolucyjne sieci neuronowe sk≥adajπ siÍ z trzech g≥Ûwnych typÛw warstw:
\begin{itemize}
	\item \textbf{Warstwa konwolucyjna (Convolutional Layer)} ñ wykonuje operacjÍ konwolucji na obrazie wejúciowym, wykrywajπc podstawowe cechy, takie jak krawÍdzie, rogi i tekstury.
	\item \textbf{Warstwa aktywacji (Activation Layer)} ñ czÍsto stosuje funkcjÍ aktywacyjnπ ReLU (Rectified Linear Unit), ktÛra wprowadza nieliniowoúÊ do sieci.
	\item \textbf{Warstwa poolingowa (Pooling Layer)} ñ zmniejsza rozmiar danych, podsumowujπc cechy w danym obszarze, co prowadzi do zmniejszenia liczby parametrÛw i zwiÍkszenia wydajnoúci obliczeniowej.
\end{itemize}

Korzenie konwolucyjnych sieci neuronowych siÍgajπ lat 80. XX wieku, kiedy Fukushima zaproponowa≥ model Neocognitron do rozpoznawania wzorcÛw odpornych na przesuniÍcia.\cite{fukushima1980} PÛüniejsze prace LeCuna i wspÛ≥autorÛw ugruntowa≥y praktyczne zastosowania CNN w zadaniach takich jak rozpoznawanie cyfr i dokumentÛw.\cite{lecun1998}

\section{Zasada dzia≥ania algorytmu YOLO i YOLOv4}

Algorytmy z rodziny \textit{YOLO} (You Only Look Once) naleøπ do najwydajniejszych rozwiπzaÒ w dziedzinie detekcji obiektÛw w czasie rzeczywistym, traktujπc to zadanie jako problem regresyjny, w ktÛrym obraz wejúciowy jest bezpoúrednio odwzorowywany na zbiÛr ramek detekcyjnych z przypisanymi klasami oraz wspÛ≥czynnikami pewnoúci~\cite{Redmon2016,Bochkovskiy2020,Goodfellow2016}. W odrÛønieniu od klasycznych detektorÛw dwuetapowych, takich jak Faster R-CNN, modele \textit{YOLO} realizujπ proces detekcji w jednym przebiegu przez sieÊ neuronowπ (\textit{one-stage detector}), co pozwala na osiπgniÍcie wysokiej szybkoúci dzia≥ania przy zachowaniu konkurencyjnej dok≥adnoúci~\cite{Redmon2016,Bochkovskiy2020}. 

W algorytmach YOLO obraz wejúciowy jest wstÍpnie przeskalowywany do ustalonej rozdzielczoúci (najczÍúciej \(416 \times 416\) lub \(608 \times 608\) pikseli), a nastÍpnie przetwarzany przez g≥Íbokπ sieÊ konwolucyjnπ, ktÛra jednoczeúnie odpowiada za ekstrakcjÍ cech, lokalizacjÍ obiektÛw oraz ich klasyfikacjÍ~\cite{Bochkovskiy2020,Geron2019}. W wyniku dzia≥ania sieci otrzymuje siÍ zbiÛr propozycji ramek ograniczajπcych (\textit{bounding boxes}) wraz z przypisanymi im prawdopodobieÒstwami przynaleønoúci do klas oraz wartoúciami ufnoúci (\textit{objectness score}), ktÛre nastÍpnie sπ filtrowane w celu uzyskania koÒcowych detekcji. YOLOv4 stanowi rozwiniÍcie wczeúniejszych wersji, wprowadzajπc szereg modyfikacji architektonicznych i technik treningowych, zapewniajπcych lepszy kompromis pomiÍdzy dok≥adnoúciπ a szybkoúciπ dzia≥ania~\cite{Bochkovskiy2020}. 

\subsection*{Etap 1: Podzia≥ obrazu na siatkÍ i definicja anchor boxes}

Podstawπ dzia≥ania algorytmÛw YOLO jest podzia≥ przeskalowanego obrazu na regularnπ siatkÍ przestrzennπ o rozmiarze \(S \times S\), gdzie \(S\) zaleøy od poziomu rozdzielczoúci detekcji~\cite{Redmon2016}. W przypadku YOLOv4 detekcja realizowana jest rÛwnoczeúnie na trzech skalach: \(13 \times 13\), \(26 \times 26\) oraz \(52 \times 52\), co umoøliwia skuteczne wykrywanie odpowiednio duøych, úrednich oraz ma≥ych obiektÛw~\cite{Bochkovskiy2020}. Kaøda komÛrka siatki odpowiada za wykrywanie obiektÛw, ktÛrych úrodek masy znajduje siÍ w jej obrÍbie, co ogranicza liczbÍ moøliwych lokalizacji i upraszcza proces optymalizacji~\cite{Geron2019}. 

Dla kaødej komÛrki siatki definiowanych jest \(B\) tzw. \textit{anchor boxes}, czyli z gÛry ustalonych propozycji ramek ograniczajπcych o okreúlonych proporcjach i rozmiarach~\cite{Bochkovskiy2020}. Anchor boxy sπ zwykle wyznaczane na podstawie analizy danych treningowych (np. metodπ k-úrednich), tak aby jak najlepiej odzwierciedla≥y typowe kszta≥ty i rozmiary obiektÛw wystÍpujπcych w danym zbiorze. W typowej konfiguracji YOLOv4 stosuje siÍ trzy anchor boxy na kaødπ komÛrkÍ dla kaødej z trzech skal, co daje ≥πcznie dziewiÍÊ anchorÛw przypisanych do danego punktu siatki i pozwala na modelowanie obiektÛw o zrÛønicowanych rozmiarach oraz proporcjach~\cite{Bochkovskiy2020}. 

\subsection*{Etap 2: Ekstrakcja cech ñ backbone CSPDarknet53}

Pierwszym etapem przetwarzania obrazu w YOLOv4 jest ekstrakcja cech wizualnych z wykorzystaniem g≥Íbokiej sieci konwolucyjnej pe≥niπcej rolÍ \textit{backbone}. W YOLOv4 funkcjÍ tÍ realizuje architektura \textit{CSPDarknet53}, bÍdπca rozwiniÍciem sieci Darknet-53 z zastosowaniem mechanizmu \textit{cross-stage partial connections}~\cite{Bochkovskiy2020}. Rozwiπzanie to pozwala na lepsze rozdzielenie przep≥ywu gradientÛw w sieci oraz redukcjÍ liczby operacji obliczeniowych bez istotnej utraty jakoúci reprezentacji cech~\cite{Bochkovskiy2020}. 

W trakcie propagacji w g≥πb sieci CSPDarknet53 obraz jest wielokrotnie poddawany operacjom konwolucji, normalizacji i nieliniowej aktywacji, co prowadzi do utworzenia hierarchicznej reprezentacji cech ñ od niskopoziomowych (krawÍdzie, tekstury) po wysokopoziomowe (struktury semantyczne). DziÍki temu kolejne warstwy sieci sπ w stanie rozrÛøniaÊ zarÛwno proste, jak i z≥oøone obiekty, co jest kluczowe dla skutecznej detekcji w rÛønorodnych scenariuszach, w tym w z≥oøonych úrodowiskach miejskich~\cite{Geron2019}. 

\subsection*{Etap 3: Agregacja wieloskalowa ñ SPP i PAN}

Po etapie ekstrakcji cech przez backbone wykorzystywane sπ modu≥y odpowiedzialne za ich dalszπ fuzjÍ i agregacjÍ. W YOLOv4 rolÍ tÍ pe≥niπ miÍdzy innymi \textit{Spatial Pyramid Pooling} (SPP) oraz \textit{Path Aggregation Network} (PAN)~\cite{Bochkovskiy2020}. Modu≥ SPP stosuje rÛwnoleg≥e operacje poolingowe o rÛønych rozmiarach okien, co pozwala na zwiÍkszenie efektywnego pola recepcji i integracjÍ informacji kontekstowych z rÛønych skal przestrzennych bez koniecznoúci zmiany wymiarÛw wejúcia~\cite{Bochkovskiy2020}. 

Z kolei \textit{Path Aggregation Network} odpowiada za efektywnπ propagacjÍ cech pomiÍdzy warstwami niøszymi i wyøszymi poprzez po≥πczenia typu top-down i bottom-up, ≥πczπc informacjÍ semantycznπ z wyøszych poziomÛw z detalami przestrzennymi z niøszych poziomÛw~\cite{Sze2017}. Taka struktura poprawia jakoúÊ detekcji ma≥ych obiektÛw oraz stabilizuje proces uczenia, poniewaø sieÊ otrzymuje bogatszπ i lepiej zrÛønicowanπ reprezentacjÍ cech na wszystkich poziomach rozdzielczoúci. W efekcie moøliwa jest jednoczesna detekcja obiektÛw o istotnie rÛønych rozmiarach przy zachowaniu wysokiej precyzji lokalizacji~\cite{Bochkovskiy2020}. 

\subsection*{Etap 4: Predykcja atrybutÛw obiektÛw na wielu skalach}

Na wyjúciu modu≥Ûw agregujπcych cechy znajdujπ siÍ g≥owy (sieci neuronowe odpowiedzialne za regresjÍ i klasyfikacjÍ) detekcyjne przypisane do trzech siatek: \(13 \times 13\), \(26 \times 26\) oraz \(52 \times 52\)~\cite{Bochkovskiy2020}. Kaøda komÛrka tych siatek generuje predykcje dla swoich anchor boxÛw. Dla kaødego anchor boxa sieÊ przewiduje zestaw parametrÛw opisujπcych potencjalny obiekt: wspÛ≥rzÍdne przesuniÍcia úrodka ramki wzglÍdem komÛrki siatki \((t_x, t_y)\), logarytmiczne przeskalowania szerokoúci i wysokoúci \((t_w, t_h)\), wspÛ≥czynnik ufnoúci \(P_{obj}\) oraz wektor prawdopodobieÒstw przypisania do poszczegÛlnych klas~\cite{Aggarwal2018}. 

Wartoúci wyjúciowe sπ nastÍpnie przekszta≥cane do przestrzeni obrazu za pomocπ funkcji nieliniowych. WspÛ≥rzÍdne úrodka ramki sπ skalowane przy uøyciu funkcji sigmoidalnej, co zapewnia ich lokalizacjÍ w obrÍbie danej komÛrki siatki, natomiast szerokoúÊ i wysokoúÊ sπ obliczane poprzez zastosowanie funkcji wyk≥adniczej do przewidywanych parametrÛw oraz pomnoøenie przez wymiary anchor boxa~\cite{Bochkovskiy2020}. DziÍki temu model uczy siÍ jedynie wzglÍdnych modyfikacji anchorÛw, co stabilizuje proces optymalizacji i poprawia zbieønoúÊ uczenia. 

Predykcja klas obiektÛw realizowana jest poprzez generowanie rozk≥adu prawdopodobieÒstwa spoúrÛd wszystkich dostÍpnych klas, zwykle przy uøyciu funkcji softmax lub sigmoidalnej, w zaleønoúci od przyjÍtej konfiguracji~\cite{Goodfellow2016}. W przypadku YOLOv4 model jest domyúlnie trenowany na zbiorze COCO, obejmujπcym 80 klas obiektÛw, takich jak osoby, pojazdy, zwierzÍta, elementy infrastruktury czy przedmioty codziennego uøytku~\cite{Lin2014}. Jednoczeúnie architektura pozwala na ≥atwe dostosowanie liczby klas do konkretnego zastosowania poprzez transfer uczenia na w≥asnym zbiorze danych. 

\subsection*{Etap 5: Filtrowanie wynikÛw ñ Non-Maximum Suppression}

Po wygenerowaniu predykcji dla wszystkich komÛrek i anchor boxÛw na trzech skalach powstaje bardzo duøy zbiÛr potencjalnych ramek detekcyjnych. Wiele z nich odnosi siÍ do tego samego obiektu, rÛøniπc siÍ nieznacznie po≥oøeniem oraz wartoúciπ wspÛ≥czynnika ufnoúci. Aby otrzymaÊ spÛjny i nieprzepe≥niony zbiÛr detekcji, stosuje siÍ algorytm \textit{Non-Maximum Suppression} (NMS)~\cite{Geron2019,Goodfellow2016}. 

Algorytm NMS polega na iteracyjnym wybieraniu ramek o najwyøszej wartoúci ufnoúci dla danej klasy, a nastÍpnie eliminowaniu tych, ktÛrych miara nak≥adania siÍ z wybranπ ramkπ, okreúlona wskaünikiem IoU (Intersection over Union), przekracza zadany prÛg~\cite{Geron2019}. W rezultacie pozostajπ jedynie ramki najlepiej reprezentujπce poszczegÛlne obiekty, co zapewnia czytelnoúÊ i jednoznacznoúÊ wynikÛw detekcji. Mechanizm ten jest szczegÛlnie istotny w scenach o wysokim zagÍszczeniu obiektÛw, gdzie wiele anchorÛw moøe wskazywaÊ na ten sam element obrazu. 

\subsection*{Etap 6: Techniki treningowe i generalizacja}

YOLOv4, oprÛcz zmian architektury, wykorzystuje takøe szereg zaawansowanych technik treningowych, takich jak \textit{mosaic data augmentation}, \textit{dropblock regularization} oraz funkcja straty \textit{CIoU loss}, ktÛre poprawiajπ zdolnoúÊ modelu do generalizacji~\cite{Bochkovskiy2020}. Mosaic augmentation polega na ≥πczeniu fragmentÛw kilku obrazÛw w jeden przyk≥ad treningowy, co zwiÍksza rÛønorodnoúÊ danych, natomiast DropBlock wprowadza losowe wyzerowywanie blokÛw aktywacji, ograniczajπc zjawisko przeuczenia~\cite{Bochkovskiy2020,Geron2019}. Z kolei CIoU loss uwzglÍdnia nie tylko nak≥adanie siÍ ramek, ale takøe odleg≥oúÊ miÍdzy ich úrodkami oraz proporcje, co prowadzi do dok≥adniejszej optymalizacji parametrÛw ramek ograniczajπcych~\cite{Bochkovskiy2020}. 

Zastosowanie tych metod powoduje, øe YOLOv4 osiπga wysokπ dok≥adnoúÊ predykcji przy zachowaniu krÛtkiego czasu inferencji, co czyni go szczegÛlnie atrakcyjnym w zastosowaniach czasu rzeczywistego, zw≥aszcza na platformach o ograniczonych zasobach obliczeniowych. Jest to kluczowe zarÛwno w systemach wbudowanych, jak i w aplikacjach wymagajπcych przetwarzania strumieni wideo na øywo. 

\subsection*{Etap 7: Zastosowania YOLO/YOLOv4, w tym integracja z CARLA}

DziÍki wysokiej szybkoúci dzia≥ania i precyzyjnej detekcji obiektÛw, YOLOv4 znajduje zastosowanie w wielu dziedzinach, takich jak systemy monitoringu wizyjnego, inteligentne miasta, analiza wideo, robotyka czy systemy bezpieczeÒstwa~\cite{yolo_usage}. SzczegÛlnie istotnπ grupÍ zastosowaÒ stanowiπ systemy wspomagania kierowcy oraz badania nad autonomicznπ jazdπ, gdzie konieczne jest niezawodne wykrywanie obiektÛw w dynamicznych i z≥oøonych scenach drogowych. 

Integracja YOLOv4 z symulatorem CARLA umoøliwia trenowanie i testowanie modeli detekcji w realistycznych, kontrolowanych warunkach miejskich, z uwzglÍdnieniem rÛønorodnych scenariuszy ruchu drogowego, zmiennych warunkÛw pogodowych oraz oúwietleniowych~\cite{carla}. W takim úrodowisku model moøe wykrywaÊ kluczowe obiekty infrastruktury drogowej, miÍdzy innymi pojazdy osobowe i ciÍøarowe, pieszych, rowerzystÛw, sygnalizacjÍ úwietlnπ oraz znaki drogowe, a takøe inne pojazdy autonomiczne~\cite{carla,yolo_usage}. Pozwala to nie tylko oceniÊ skutecznoúÊ samej detekcji, lecz takøe testowaÊ algorytmy podejmowania decyzji, planowania trajektorii oraz unikania kolizji, co ma kluczowe znaczenie w rozwoju systemÛw autonomicznej jazdy. 

\subsection{Architektura YOLOv4}

YOLOv4 jest jednπ z najnowszych wersji modelu YOLO, ktÛry jest jednym z najpopularniejszych algorytmÛw do detekcji obiektÛw w obrazach. Jego architektura opiera siÍ na trzech g≥Ûwnych komponentach:
\begin{itemize}
	\item \textbf{Backbone} - jest odpowiedzialny za ekstrakcjÍ cech z obrazu. W YOLOv4 wykorzystano zaawansowanπ sieÊ ResNet-50, ktÛra pozwala na szybkie i dok≥adne przetwarzanie obrazu.
	\item \textbf{Neck} - ≥πczy cechy z rÛønych warstw backbone i pomaga w ich dalszym przetwarzaniu, umoøliwiajπc detekcjÍ obiektÛw w rÛønych skalach. W YOLOv4 zastosowano PANet (Path Aggregation Network), ktÛre poprawia reprezentacjÍ cech.
	\item \textbf{Head} - dokonuje finalnej klasyfikacji oraz lokalizacji obiektÛw na obrazie, przy pomocy detekcji boxÛw i klasyfikacji dla kaødego wykrytego obiektu.
\end{itemize}

YOLOv4 korzysta z zaawansowanych technik, takich jak:
\begin{itemize}
	\item \textbf{DropBlock} - technika regularizacji, ktÛra pomaga zapobiegaÊ przeuczeniu (overfitting).
	\item \textbf{CSPDarknet53} - nowoczesna sieÊ, ktÛra stanowi podstawÍ (backbone) YOLOv4.
	\item \textbf{CIoU Loss} - funkcja straty, ktÛra poprawia dok≥adnoúÊ lokalizacji obiektÛw.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial3/yolov4_architecture.png}
	\caption{Architektura YOLOv4. Zawiera backbone, neck i head~\cite{yolo_usage}.}
	\label{fig:yolov4_arch}
\end{figure}

Wszystkie te elementy wspÛ≥pracujπ, aby umoøliwiÊ YOLOv4 wykrywanie obiektÛw na obrazach w czasie rzeczywistym, przy zachowaniu wysokiej dok≥adnoúci i wydajnoúci. DziÍki zastosowaniu wielu technik optymalizacyjnych, YOLOv4 osiπga bardzo wysokπ dok≥adnoúÊ i duøπ szybkoúÊ dzia≥ania w porÛwnaniu z poprzednimi wersjami.

\section{Zalety, wydajnoúÊ i zastosowania algorytmu YOLOv4}

YOLOv4 (You Only Look Once version 4) to jedna z najnowoczeúniejszych i najbardziej zaawansowanych wersji sieci neuronowych przeznaczonych do detekcji obiektÛw w czasie rzeczywistym. Algorytm ten wyrÛønia siÍ znakomitπ rÛwnowagπ pomiÍdzy szybkoúciπ dzia≥ania a jakoúciπ detekcji, co sprawia, øe z powodzeniem znajduje zastosowanie zarÛwno w badaniach naukowych, jak i w aplikacjach przemys≥owych, militarnych czy cywilnych.

\subsection*{Zalety i efektywnoúÊ dzia≥ania}

YOLOv4 ≥πczy w sobie liczne usprawnienia architektoniczne i techniczne wzglÍdem wczeúniejszych wersji (YOLOv1ñYOLOv3) oraz konkurencyjnych metod takich jak Faster R-CNN czy SSD. Do jego najistotniejszych zalet naleøπ:

\begin{itemize}
	\item \textbf{Wysoka prÍdkoúÊ dzia≥ania} ñ osiπga nawet do 65 klatek na sekundÍ (FPS) na wydajnych procesorach graficznych, co pozwala na detekcjÍ w czasie rzeczywistym \cite{Bochkovskiy2020}.
	\item \textbf{Obs≥uga urzπdzeÒ brzegowych} ñ dziÍki optymalizacji sieci i wsparciu dla technologii takich jak TensorRT, YOLOv4 moøe dzia≥aÊ na urzπdzeniach o ograniczonej mocy obliczeniowej (np. Nvidia Jetson).
	\item \textbf{Wysoka dok≥adnoúÊ detekcji} ñ osiπga wynik mAP (mean Average Precision) rzÍdu 43,5\% na zestawie danych COCO, co czyni go jednym z liderÛw wúrÛd modeli jednoetapowych (ang. \textit{one-stage detectors}) \cite{Bochkovskiy2020}.
	\item \textbf{ElastycznoúÊ i adaptowalnoúÊ} ñ model moøna z ≥atwoúciπ dostosowaÊ do nowych klas obiektÛw poprzez proces tzw. \textit{fine-tuningu}.
	\item \textbf{OdpornoúÊ na zak≥Ûcenia} ñ YOLOv4 radzi sobie z trudnymi warunkami detekcji, takimi jak czÍúciowe zas≥oniÍcia, rotacje, zmienne oúwietlenie czy szum.
\end{itemize}

\subsection*{Inne przyk≥ady zastosowania algorytmu YOLO}

Model ten znajduje zastosowanie w szerokim zakresie dziedzin, m.in.:

\begin{itemize}
	\item \textbf{Monitorowanie i analiza ruchu drogowego} ñ YOLOv4 skutecznie identyfikuje pojazdy, pieszych, rowerzystÛw, znaki drogowe i inne elementy infrastruktury drogowej, umoøliwiajπc zastosowanie w systemach ITS (Intelligent Transportation Systems) \cite{yolo_usage}.
	\item \textbf{Systemy bezpieczeÒstwa i nadzoru wideo} ñ detekcja osÛb, bagaøy, podejrzanych zachowaÒ czy naruszeÒ przestrzeni publicznych.
	\item \textbf{Automatyka przemys≥owa} ñ wykrywanie defektÛw, klasyfikacja produktÛw oraz inspekcja wizualna na liniach produkcyjnych.
	\item \textbf{Robotyka i pojazdy autonomiczne} ñ rozpoznawanie przeszkÛd i elementÛw otoczenia w czasie rzeczywistym w celu bezpiecznej nawigacji.
\end{itemize}

\section{Instalacja systemu YOLO}
\subsection*{Instalacja wymaganych pakietÛw}

Pierwszym krokiem w procesie instalacji jest zainstalowanie wszystkich wymaganych pakietÛw, w tym Git, Python oraz bibliotek zwiπzanych z CUDA i OpenCV. W celu zaktualizowania listy pakietÛw naleøy wykonaÊ poniøsze polecenie:

\begin{lstlisting}[caption={Aktualizacja listy pakietÛw}]
	sudo apt update
\end{lstlisting}

NastÍpnie zainstalowane zostanπ wymagane pakiety:

\begin{lstlisting}[caption={Instalacja wymaganych pakietÛw}]
	sudo apt install build-essential cmake git pkg-config libjpeg8-dev \
	libtiff5-dev libjasper-dev libpng12-dev libopencv-dev libeigen3-dev \
	libatlas-base-dev gfortran python3-dev python3-pip python3-numpy \
	libhdf5-dev libhdf5-serial-dev libprotobuf-dev protobuf-compiler \
	libgflags-dev libgoogle-glog-dev liblmdb-dev
\end{lstlisting}

\subsection*{Instalacja CUDA i cuDNN}

W przypadku chÍci korzystania z przyspieszenia GPU, konieczna jest instalacja CUDA oraz cuDNN.

Ay zainstalowaÊ odpowiedniπ wersjÍ CUDA, zgodnπ z systemem, naleøy pobraÊ jπ ze strony NVIDIA: \url{https://developer.nvidia.com/cuda-downloads}. W celu zainstalowania CUDA naleøy wykonaÊ poniøsze polecenie:

\begin{lstlisting}[caption={Instalacja CUDA}]
	sudo apt install nvidia-cuda-toolkit
\end{lstlisting}

Aby zainstalowaÊ cuDNN, ktÛry jest niezbÍdny do przyspieszenia obliczeÒ na GPU, naleøy wykonaÊ poniøsze polecenie:

\begin{lstlisting}[caption={Instalacja cuDNN}]
	sudo apt install libcudnn7 libcudnn7-dev
\end{lstlisting}

\subsection*{Klonowanie repozytorium YOLOv4 i kompilacja}

Aby pobraÊ kod ürÛd≥owy YOLOv4, oficjalne repozytorium z GitHub jest klonowane przy uøyciu poniøszego polecenia:

\begin{lstlisting}[caption={Klonowanie repozytorium YOLOv4}]
	git clone https://github.com/AlexeyAB/darknet
	cd darknet
\end{lstlisting}

NastÍpnie plik \texttt{Makefile} naleøy edytowaÊ, aby w≥πczyÊ obs≥ugÍ CUDA (GPU) oraz OpenCV, zmieniajπc odpowiednie opcje na:

\begin{lstlisting}[caption={Edytowanie pliku Makefile}]
	GPU=1
	CUDNN=1
	OPENCV=1
\end{lstlisting}

Po dokonaniu zmian, projekt jest kompilowany za pomocπ polecenia:

\begin{lstlisting}[caption={Kompilacja projektu YOLOv4}]
	make
\end{lstlisting}

\subsection{Testowanie instalacji}

Po zakoÒczeniu kompilacji system moøe zostaÊ przetestowany, aby upewniÊ siÍ, øe instalacja przebieg≥a pomyúlnie. YOLOv4 moøe zostaÊ uruchomiony na przyk≥adowym obrazie przy uøyciu poniøszego polecenia:

\begin{lstlisting}[caption={Testowanie YOLOv4}]
	./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights data/dog.jpg
\end{lstlisting}

Jeúli wszystko zosta≥o poprawnie zainstalowane, powinien zostaÊ wyúwietlony wynik wykrywania obiektÛw na obrazie.

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial3/yolo_test.png}
	\label{fig:yoloTest}
	\caption[Yolo Test]{Obraz przedstawiajπcy poprawne zainstalowanie i uruchomienie YOLOv4.}
\end{figure}

\section{Integracja detektora YOLOv4 z symulatorem CARLA}

W celu rozszerzenia funkcjonalnoúci symulatora CARLA o moøliwoúÊ detekcji obiektÛw w czasie rzeczywistym, dokonano integracji modelu YOLOv4 z plikiem \texttt{manual\_control.py}. Model YOLOv4 (You Only Look Once) to zaawansowany algorytm detekcji obiektÛw w obrazie, ktÛry umoøliwia identyfikacjÍ oraz lokalizacjÍ wielu klas obiektÛw w pojedynczym przebiegu sieci neuronowej.

Proces integracji rozpoczÍto od zaimportowania wymaganych bibliotek i konfiguracji úrodowiska TensorFlow:

\begin{lstlisting}[style=pythonColor, emph={import tensorflow, InteractiveSession, filter\_boxes, cfg}, caption={Importowanie bibliotek dla integracji z YOLOv4}]
	import tensorflow as tf
	from tensorflow.compat.v1 import InteractiveSession
	from core.yolov4 import filter_boxes
	from core.config import cfg
\end{lstlisting}

Zdefiniowana zosta≥a rÛwnieø globalna funkcja \verb|spawn_actor()|, ktÛrej celem jest wczytywanie i przekszta≥canie listy wspÛ≥rzÍdnych tzw. \verb|anchor boxes|, bÍdπcych podstawπ w modelu YOLO do przewidywania po≥oøenia obiektÛw w obrazie. Przyjmuje jako argument listÍ wspÛ≥rzÍdnych opisujπcych wymiary anchorÛw, nastÍpnie przekszta≥ca jπ do struktury trÛjwymiarowej, umoøliwiajπcej przypisanie anchorÛw do trzech skal detekcji, z ktÛrych kaøda operuje na prostokπtnych propozycjach. DziÍki takiej organizacji danych moøliwe jest skuteczne dopasowanie anchorÛw do charakterystyki obiektÛw wystÍpujπcych w obrazie, co znaczπco wp≥ywa na jakoúÊ predykcji oraz efektywnoúÊ dzia≥ania algorytmu detekcji.

\begin{lstlisting}[language=Python, style=pythonColor, emph={get\_anchors, array, reshape}, caption={Globalna funkcja \texttt{get\_anchors()} dla YOLOv4}]
	def get_anchors(anchors_path):
	anchors = np.array(anchors_path)
	return anchors.reshape(3, 3, 2)
\end{lstlisting}

Po wczytaniu obrazu z symulatora przy uøyciu biblioteki \texttt{pygame}, ramka obrazu jest skalowana i przekszta≥cana na odpowiedni format:

\begin{lstlisting}[style=pythonColor, emph={pygame.surfarray.array3d, cv2.resize, np.newaxis}, caption={Wczytywanie i przetwarzanie obrazu z symulatora CARLA}]
	frame = pygame.surfarray.array3d(display)
	image_data = cv2.resize(frame, (self.input_size, self.input_size))
	image_data = image_data / 255.
	image_data = image_data[np.newaxis, ...].astype(np.float32)
\end{lstlisting}

Obraz ten trafia nastÍpnie do sieci neuronowej YOLOv4, ktÛra zwraca ramki ograniczajπce (ang. bounding boxes), prawdopodobieÒstwa detekcji oraz klasy obiektÛw. Przyk≥adowo, wykorzystano funkcjÍ \texttt{combined\_non\_max\_suppression} do eliminacji powtarzajπcych siÍ wykryÊ:

\begin{lstlisting}[style=pythonColor, emph={tf.image.combined\_non\_max\_suppression}, caption={Wykorzystanie funkcji \texttt{combined\_non\_max\_suppression} w celu eliminacji powtÛrzeÒ}]
	boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(...)
\end{lstlisting}

Tak przygotowane dane sπ nastÍpnie przetwarzane i wizualizowane w úrodowisku symulatora CARLA.

\section{Schemat ewaluacji offline}

OprÛcz testÛw on-line, w ktÛrych YOLOv4 dzia≥a≥ bezpoúrednio w symulatorze CARLA, opracowano rÛwnieø schemat ewaluacji offline. Jego celem by≥o szczegÛ≥owe porÛwnanie rezultatÛw detekcji sieci z danymi referencyjnymi (\textit{ground truth}) generowanymi przez symulator, z wykorzystaniem metryki Intersection over Union (IoU) \cite{carla,visoiou}. 

Proponowana procedura sk≥ada siÍ z kilku kolejnych etapÛw.
\begin{enumerate}
	\item \textbf{Generowanie danych referencyjnych w CARLA.}  
	Podczas symulacji rejestrowane sπ klatki z kamery RGB zamontowanej na egoñpojeüdzie oraz odpowiadajπce im informacje o aktorach w scenie. Skrypt \texttt{bounding\_boxes.py} odczytuje strukturÍ \texttt{carla.BoundingBox} dla kaødego pojazdu, przelicza wierzcho≥ki bry≥y 3D do uk≥adu wspÛ≥rzÍdnych kamery i rzutuje je na p≥aszczyznÍ obrazu, wyznaczajπc ostatecznie prostokπtne obramowanie 2D (bbox 2D). Dla kaødej klatki zapisywany jest plik JSON zawierajπcy listÍ obiektÛw z nazwπ klasy, wspÛ≥rzÍdnymi prostokπta w pikselach oraz identyfikatorem klatki/obrazu, ktÛry pozwala jednoznacznie powiπzaÊ anotacje z konkretnym plikiem JPG. 
	
	\item \textbf{Detekcja obiektÛw za pomocπ YOLOv4.}  
	W drugim kroku YOLOv4, zaimplementowane w bibliotece Ultralytics, jest uruchamiane w trybie wsadowym na zestawie obrazÛw zapisanych wczeúniej z kamery symulatora. Dla kaødego pliku JPG model zwraca listÍ wykrytych obiektÛw wraz z klasπ, wspÛ≥czynnikiem pewnoúci (ang. \textit{confidence}) oraz wspÛ≥rzÍdnymi proponowanego bounding boxa 2D. Wyniki sπ eksportowane do osobnego pliku JSON, w ktÛrym kaøda detekcja zawiera: nazwÍ obrazu, nazwÍ klasy, wspÛ≥rzÍdne prostokπta i wartoúÊ confidence \cite{ultralytics_docs,ultralytics_python}. 
	
	\item \textbf{Parowanie detekcji z anotacjami CARLA.}  
	Skrypt ewaluacyjny wczytuje rÛwnolegle pliki JSON z anotacjami CARLA oraz pliki z predykcjami YOLOv4. Dla kaødej klatki obrazu wyszukiwane sπ pary prostokπtÛw: jeden pochodzπcy z CARLA (ground truth) oraz drugi wygenerowany przez YOLOv4, naleøπce do tej samej klasy obiektu (np. \textit{car}). NastÍpnie dla kaødej moøliwej pary obliczana jest wartoúÊ IoU, a dopasowanie wybierane jest na podstawie najwiÍkszej wartoúci IoU powyøej zadanego progu (np. 0{,}5). Pozwala to zapobiec sytuacjom, w ktÛrych wiele predykcji przypisano by do tego samego obiektu. 
	
	\item \textbf{Obliczanie metryk jakoúciowych.}  
	Na etapie koÒcowym skrypt zlicza poprawne dopasowania (TP ñ \textit{true positive}), pominiÍte obiekty (FN ñ \textit{false negative}) oraz nadmiarowe detekcje YOLOv4 (FP ñ \textit{false positive}). Na tej podstawie wyznaczane sπ statystyki jakoúciowe, takie jak úrednia wartoúÊ IoU, precyzja (precision) czy czu≥oúÊ (recall) dla wybranych scen i warunkÛw pogodowych. Dodatkowo zapisywane sπ rozk≥ady IoU (np. histogramy), co pozwala zidentyfikowaÊ przypadki skrajne ñ bardzo dobre oraz bardzo s≥abe dopasowania. Wyniki te stanowiπ podstawÍ do analizy wp≥ywu warunkÛw oúwietleniowych i atmosferycznych na dok≥adnoúÊ detekcji YOLOv4. 
\end{enumerate}

Zaproponowany schemat ewaluacji offline umoøliwia powtarzalnπ, zautomatyzowanπ ocenÍ jakoúci dzia≥ania modelu YOLOv4 na danych generowanych w symulatorze CARLA. Oddzielenie etapu generowania klatek od etapu detekcji i ewaluacji u≥atwia rÛwnieø dalsze eksperymentowanie, np. z innymi wersjami modelu YOLO, dodatkowymi klasami obiektÛw czy zmodyfikowanymi ustawieniami kamery, bez koniecznoúci ponownego wykonywania kosztownych symulacji. 
