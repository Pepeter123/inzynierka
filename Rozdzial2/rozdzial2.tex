\chapter{Symulator CARLA}

CARLA (Car Learning to Act) jest otwartym symulatorem jazdy miejskiej, przeznaczonym do wspierania szkoleñ, treningów, tworzenia prototypów oraz walidacji autonomicznych systemów jazdy zarówno na poziomie percepcji, jak i kontroli. System stanowi otwart¹ platformê stworzon¹ przez specjalistyczny zespó³ grafików i in¿ynierów, obejmuj¹c¹ uk³ady urbanistyczne, modele pojazdów, budynki, pieszych oraz znaki drogowe. Symulacja umo¿liwia elastyczn¹ konfiguracjê scenariuszy, pozyskiwanie wspó³rzêdnych GPS, prêdkoœci i przyspieszeñ pojazdów, a tak¿e informacji o kolizjach i wykroczeniach drogowych. Œrodowisko pozwala równie¿ na modyfikacjê warunków pogodowych i pory dnia, co u³atwia testowanie algorytmów w zró¿nicowanych warunkach ruchu~\cite{carla}.

%=================================================================================================
\section{Architektura systemu CARLA}

Symulator CARLA bazuje na silniku graficznym Unreal Engine 4 (UE4), który odpowiada za realistyczne odwzorowanie œrodowiska symulacyjnego, w tym geometrii œwiata, pojazdów, pieszych oraz warunków atmosferycznych~\cite{unreal-engine}. Na sil\textsf{}niku UE4 zbudowana jest skalowalna architektura typu klient–serwer, stanowi¹ca podstawê dzia³ania symulatora. 

Serwer odpowiada za wszystkie elementy zwi¹zane z przebiegiem symulacji, w szczególnoœci za renderowanie œwiata i aktorów, obliczanie zjawisk fizycznych oraz generowanie pomiarów z czujników. Z kolei strona klienta sk³ada siê z modu³ów kontroluj¹cych logikê aktorów na scenie i konfiguruj¹cych warunki panuj¹ce w œwiecie. Komunikacja miêdzy klientem a serwerem odbywa siê za pomoc¹ interfejsu API CARLA (dostêpnego m.in. w Pythonie i C++), który jest stale rozwijany, aby udostêpniaæ nowe funkcje~\cite{carla}.

Poni¿ej przedstawiono wybrane elementy systemu:

\begin{itemize}
	\item Mened¿er ruchu, czyli wbudowany system, który przejmuje kontrole nad pojazdami. Dzia³a jako przewodnik dostarczony przez CARLA do odtworzenia œrodowisk miejskich z realistycznymi zachowaniami.
	\item Czujniki, na których polegaj¹ pojazdy podczas przekazywania informacji o swoim otoczeniu. S¹ to specyficzni aktorzy pod³¹czeni do pojazdu a dane, które otrzymuj¹ mog¹ byæ przechowywane i wyszukiwane w celu u³atwienia procesu. Obecnie projekt obs³uguje ich ró¿ne typy - kamery, radary, lidary, itd.
	\item Rejestrator, czyli funkcja s³u¿¹ca do odtwarzania symulacji krok po kroku dla ka¿dego aktora na œwiecie. Dziêki niej u¿ytkownik ma dostêp do ka¿dego miejsca na œwiecie w dowolnym momencie osi czasu. 
	\item Most ROS i implementacja Autoware, które zapewni¹ integracjê symulatora z innymi œrodowiskami do uczenia maszynowego i testowania jazdy autonomicznej.
	\item Otwarte zasoby, czyli u³atwienie tworzenia ró¿nych map miejskich z kontrol¹ warunków pogodowych i bibliotek¹ planów miast z szerokim zestawem aktorów do wykorzystania. 
	\item Scenariusz jazdy. Aby u³atwiæ proces uczenia i testowania pojazdów autonomicznych, CARLA zapewnia seriê tras opisuj¹cych ró¿ne sytuacje, które mog¹ byæ wielokrotnie wykorzystywane w kolejnych iteracjach eksperymentów. Scenariusze te stanowi¹ równie¿ podstawê wyzwañ CARLA (CARLA Challenge), otwartych dla wszystkich u¿ytkowników zainteresowanych ewaluacj¹ w³asnych rozwi¹zañ oraz porównaniem ich wyników w publicznych rankingach.
\end{itemize}

\section{Mo¿liwoœci symulatora}
CARLA wykorzystywana jest do badania trzech podejœæ do autonomicznej jazdy:

I. Podejœcie klasyczne (modu³owe) – obejmuj¹ce potok przetwarzania sk³adaj¹cy siê z modu³u percepcji opartego na danych wizyjnych, planera trajektorii bazuj¹cego na regu³ach oraz modu³u sterowania realizuj¹cego manewry pojazdu.

II. Podejœcie oparte na uczeniu naœladowczym (imitation learning) – wykorzystuj¹ce g³êbok¹ sieæ neuronow¹, która na podstawie danych sensorycznych generuje polecenia steruj¹ce, ucz¹c siê zachowañ kierowców na podstawie zarejestrowanych przyk³adów.

III. Podejœcie end-to-end z uczeniem ze wzmocnieniem – wykorzystuj¹ce rozbudowan¹ sieæ neuronow¹ trenowan¹ od pocz¹tku do koñca w œrodowisku symulacyjnym z zastosowaniem metod uczenia ze wzmocnieniem.

Symulator Carla oferuje u¿ytkownikowi mo¿liwoœci konfiguracji œrodowiska, w którym odbywa siê symulacja. Miêdzy innymi utworzenie w³asnej mapy z elementami w postaci budynków, pojazdów oraz pieszych, czy te¿ korzystanie z gotowych œrodowisk utworzonych przez autorów projektu. Œrodowisko umo¿liwia symulowanie warunków pogodowych, oraz sterowanie sygnalizacj¹ œwietln¹ za pomoc¹ funkcji opisanych w jêzyku Python. W œrodowisku Carla, zaimplementowane s¹ tak¿e sensory, które odgrywaj¹ istotn¹ rolê w przypadku kolizji czy te¿ ustawiania atrybutów kamery. 

\subsection{Œwiat}


Mapa jest jednym z g³ównych elementów œwiata symulatora. Zawiera zarówno model 3D miejscowoœci, jak i definicjê dróg. Definicja dróg na mapie oparta jest na pliku OpenDRIVE, standardowym formacie definicji dróg z adnotacjami. Sposób, w jaki standard 1.4 OpenDRIVE definiuje drogi, pasy ruchu, skrzy¿owania itp. wp³ywa na funkcjonalnoœæ interfejsu API w jêzyku Python.

API Python dzia³a jako wysokopoziomowy system zapytañ do nawigacji po tych drogach. Jest ono stale rozwijane, aby zapewniæ szerszy zestaw narzêdzi.

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial2/map_1.png}
	\caption[Domyœlna mapa -  Town01]{Obraz mapy domyœlnej - Town01.}
	\label{fig:mapaTown01}
\end{figure} 

Domyœln¹ map¹ symulatora CARLA jest Town01 Rys. \ref{fig:mapaTown01}, bêd¹ca odwzorowaniem centrum du¿ego miasta. Program oferuje wiele gotowych map, z których mo¿na korzystaæ w celu przeprowadzenia eksperymentów, nie tylko podczas ró¿nych warunków pogodowych, ale równie¿ w rozmaitych œrodowiskach. Przyk³adem jest mapa Town04 Rys. \ref{fig:mapaTown04}, przedstawiaj¹ca ma³e miasteczko w górach, przy jeziorze.

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial2/Town04.png}
	\caption[Mapa Town04]{Obraz mapy - Town04.}
	\label{fig:mapaTown04}
\end{figure} 

Œwiat symulatora zapewnia równie¿ aktorów. Aktorami s¹ nie tylko pojazdy i piesi, ale tak¿e czujniki, znaki drogowe, sygnalizacja œwietlna i widzowie. Bardzo wa¿ne jest, aby mieæ pe³ne zrozumienie, jak na nich operowaæ. Istnieje mo¿liwoœæ tworzenia aktorów zarówno rêczenia, poprzez funkcjê \verb|spawn_actor()| jak i z wykorzystaniem przyk³adowego programu zapewnionego przez developerów - \verb|generate_traffic.py|

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial2/traffic.png}
	\label{fig:korek}
	\caption[Korek drogowy]{Obraz przedstawiaj¹cy symulacjê 30 pojazdów oraz 10 pieszych.}
\end{figure} 


\subsection{Sensory}


Sensor kolizji jest to czujnik, który rejestruje zdarzenie za ka¿dym razem, gdy jego aktor macierzysty zderzy siê z czymœ w œwiecie. Podczas jednego kroku symulacji mo¿e zostaæ wykrytych kilka kolizji. Aby zapewniæ wykrywanie kolizji z dowolnym obiektem, serwer tworzy ,,fa³szywych'' aktorów dla elementów takich jak budynki czy krzewy, dziêki czemu mo¿liwe jest pobranie znacznika semantycznego w celu ich identyfikacji.

Detektory kolizji nie posiadaj¹ ¿adnych konfigurowalnych atrybutów. \\


\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|l|} 
		\hline
		Atrybuty        & Typ             & Opis                                                                                                            \\ 
		\hline
		\verb|frame|           & int             & Numer ramki, w której dokonano pomiaru.                                                                         \\ 
		\hline
		\verb|timestamp|       & double          & \begin{tabular}[c]{@{}l@{}}Czas symulacji pomiaru w\\sekundach od jej pocz¹tku.\end{tabular}                    \\ 
		\hline
		\verb|transform|       & carla.Transform & \begin{tabular}[c]{@{}l@{}}Po³o¿enie i obrót we wspó³rzêdnych \\œwiata czujnika w czasie pomiaru.\end{tabular}  \\ 
		\hline
		\verb|actor|           & carla.Actor     & \begin{tabular}[c]{@{}l@{}}Aktor, który zmierzy³ kolizjê \\(rodzic czujnika).\end{tabular}                      \\ 
		\hline
		\verb|other_actor|    & carla.Actor     & Aktor, z którym zderzy³ siê rodzic.                                                                             \\ 
		\hline
		\verb|normal_impulse| & carla.Vector3D  & Normalny impuls wynikaj¹cy z kolizji.                                                                           \\
		\hline
	\end{tabular}
	\caption{Atrybuty sensora kolizji}
\end{table}

\begin{figure}[h!]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial2/collision_example.png}
	\label{fig:przyklad3D}
	 \caption[Sensor odpowiadaj¹cy za kolizje]{Przyk³adowy obraz kolizji ze s³upem.}
\end{figure}

Kolejnym sensorem jest kamera g³êbi. Kamera dostarcza surowe dane sceny koduj¹ce odleg³oœæ ka¿dego piksela od kamery (znane równie¿ jako bufor g³êbi lub z-bufor) w celu stworzenia mapy g³êbi elementów.

Obraz koduje wartoœæ g³êbi na piksel u¿ywaj¹c 3 kana³ów przestrzeni kolorów RGB, od mniej do bardziej znacz¹cych bajtów: R -> G -> B. Rzeczywista odleg³oœæ w metrach mo¿e byæ zdekodowana za pomoc¹: \newline

\begin{lstlisting}[style=praca]
	normalized = (R + G * 256 + B * 256 * 256) / (256 * 256 * 256 - 1)
	in_meters = 1000 * normalized
\end{lstlisting}


\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial2/depth_camera_oryginal.png}
	\label{fig:przyklad3D}
	\caption[Kamera g³êbi orygina³]{Oryginalny obraz pochodz¹cy z kamery g³êbi.}
\end{figure}

Wyjœciowy obraz CARLA.Image powinien zostaæ zapisany na dysk przy u¿yciu \verb|carla.colorConverter|, który zamieni odleg³oœæ zapisan¹ w kana³ach RGB na [0,1] float zawieraj¹cy odleg³oœæ, a nastêpnie przet³umaczy to na skalê szaroœci. Istniej¹ dwie opcje w \verb|carla.colorConverter|, aby uzyskaæ widok g³êbi: g³êbia w odcieniach szaroœci oraz g³êbokoœæ logarytmiczna. Precyzja jest milimetrowa w obu, ale podejœcie logarytmiczne zapewnia lepsze wyniki dla bli¿szych obiektów. Ponadto widocznoœæ jest lepsza dla u¿ytkownika.

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial2/depth_gray_scale.png}
	\label{fig:przyklad3D}
	\caption[Kamera g³êbi po konwersji]{Obraz pochodz¹cy z kamery g³êbi po konwersji w odcieniach szaroœci.}
\end{figure} 


\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial2/depth_logarytmic.png}
	\label{fig:przyklad3D}
	\caption[Kamera g³êbi logarytmiczna]{Obraz pochodz¹cy z kamery g³êbi - logarytmiczny.}
\end{figure} 


Sensor GNSS (Global Navigation Satellite Systems) - podaje aktualn¹ pozycjê GNSS swojego obiektu nadrzêdnego. Jest ona obliczana poprzez dodanie pozycji metrycznej do pocz¹tkowej lokalizacji georeferencyjnej zdefiniowanej w definicji mapy OpenDRIVE

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial2/gnss.png}
	\label{fig:gnss}
	\caption[Sensor GNSS]{Widok sensora GNSS.}
\end{figure} 

Sensor IMU dostarcza natomiast miary, które akcelerometr, ¿yroskop i kompas mog³yby pobraæ dla obiektu nadrzêdnego. Dane s¹ pobierane z bie¿¹cego stanu obiektu.

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial2/imu.png}
	\label{fig:imu}
	\caption[Sensor IMU]{Widok sensora IMU.}
\end{figure} 

Detektor wtargniêcia na pas ruchu rejestruje zdarzenie za ka¿dym razem, gdy jego rodzic przekroczy oznaczenie pasa ruchu. Czujnik wykorzystuje dane drogowe dostarczane przez opis mapy OpenDRIVE, aby okreœliæ, czy pojazd macierzysty naje¿d¿a na inny pas ruchu, bior¹c pod uwagê przestrzeñ miêdzy ko³ami. Nale¿y jednak zwróciæ uwagê na nastêpuj¹ce kwestie:

Rozbie¿noœci pomiêdzy plikiem OpenDRIVE a map¹ spowoduj¹ powstanie nieprawid³owoœci, takich jak przecinaj¹ce siê pasy ruchu, które nie s¹ widoczne na mapie.
Wyjœcie pobiera listê oznaczeñ przecinaj¹cych siê pasów: obliczenia s¹ wykonywane w OpenDRIVE i uwzglêdniaj¹ ca³¹ przestrzeñ pomiêdzy czterema ko³ami jako ca³oœæ. W zwi¹zku z tym, w tym samym czasie mo¿e byæ przekraczanych wiêcej ni¿ jeden pas ruchu.

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial2/crossed_line.png}
	\label{fig:crossedline}
	\caption[Sensor pDrzeciêcia linii]{Obraz przyk³adu przeciêcia linii.}
\end{figure} 

Sensor LIDAR (eng. Light Detection and Ranging) -czujnik symuluj¹cy obrotowy skaner laserowy, w którym generowanie pomiarów realizowane jest z wykorzystaniem metody ray castingu. Punkty generowane s¹ poprzez emisjê promieni laserowych dla ka¿dego kana³u, rozmieszczonych w zakresie pionowego pola widzenia (FOV). Obrót jest symulowany poprzez obliczenie k¹ta poziomego, o jaki obróci³ siê LIDAR w danej klatce. Chmura punktów generowana jest poprzez wykonanie operacji ray castingu dla ka¿dego promienia lasera w ka¿dym kroku symulacji.

Pomiar LIDAR-owy zawiera paczkê z wszystkimi punktami wygenerowanymi w przedziale czasu 1/FPS. Podczas tego interwa³u fizyka nie jest aktualizowana, wiêc wszystkie punkty w pomiarze odzwierciedlaj¹ ten sam "statyczny obraz" sceny.

Informacja z pomiaru LIDAR-owego jest zakodowana w postaci punktów 4D. Pierwsze trzy z nich to punkty przestrzenne we wspó³rzêdnych xyz, a ostatni to straty intensywnoœci podczas jazdy. Intensywnoœæ ta jest obliczana wed³ug nastêpuj¹cego wzoru:

\begin{equation}
	\frac{I}{I_0} = e^{-a \cdot d}
\end{equation}

Gdzie:

\begin{description}
	\item[$a$] -- Oznacza wspó³czynnik t³umienia. Mo¿e on zale¿eæ od d³ugoœci fali czujnika oraz warunków atmosferycznych. Mo¿na go zmodyfikowaæ za pomoc¹ atrybutu LIDAR \verb|atmosphere_attenuation_rate|.
	\item[$b$] -- Odleg³oœæ od punktu trafienia do czujnika.
\end{description}

W celu zwiêkszenia realizmu symulacji LIDAR umo¿liwia losowe odrzucanie punktów chmury (general drop-off), co pozwala modelowaæ straty pomiarowe oraz poprawiæ wydajnoœæ obliczeniow¹. Ustawienie parametru na wartoœæ 0,5 powoduje odrzucenie 50\% punktów.
\begin{lstlisting}[style=praca]
	dropoff\_general\_rate = 0.5)
\end{lstlisting}

Drugim mechanizmem jest zrzucanie punktów zale¿ne od intensywnoœci odbicia. Dla ka¿dego wykrytego punktu wykonywana jest dodatkowa operacja odrzucenia z prawdopodobieñstwem wyznaczanym na podstawie obliczonej intensywnoœci sygna³u. Prawdopodobieñstwo to definiowane jest przez dwa parametry: \verb|dropoff_zero_intensity|, okreœlaj¹cy prawdopodobieñstwo odrzucenia punktów o zerowej intensywnoœci, oraz \verb|dropoff_intensity_limit|,  wyznaczaj¹cy próg intensywnoœci, powy¿ej którego punkty nie s¹ odrzucane. W zakresie pomiêdzy tymi wartoœciami prawdopodobieñstwo zrzucenia punktu zmienia siê liniowo jako funkcja intensywnoœci. \\

Dodatkowo, atrybut \verb|noise_stddev| tworzy model szumu, aby symulowaæ nieoczekiwane odchylenia, które pojawiaj¹ siê w rzeczywistych czujnikach. Dla wartoœci dodatnich, ka¿dy punkt jest losowo zak³ócany wzd³u¿ wektora promienia lasera. W rezultacie otrzymujemy czujnik LIDAR z doskona³ym pozycjonowaniem k¹towym, ale zaszumionym pomiarem odleg³oœci.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{Rysunki/Rozdzial2/lidar.png}
	\label{fig:SensorLIDAR}
	\caption[Sensor LIDAR]{Obraz sensora LIDAR.}
\end{figure} 

Detektor przeszkód rejestruje zdarzenie za ka¿dym razem, gdy aktor macierzysty ma przed sob¹ przeszkodê. W celu przewidywania przeszkód, czujnik tworzy przed pojazdem macierzystym kszta³t kapsu³y i wykorzystuje go do sprawdzania, czy nie dochodzi do kolizji. Aby zapewniæ wykrywanie kolizji z ka¿dym rodzajem obiektu, serwer tworzy "fa³szywych" aktorów dla elementów takich jak budynki lub krzewy, dziêki czemu mo¿na pobraæ znacznik semantyczny w celu ich identyfikacji.

\begin{figure}[H]
	\centering
	\includegraphics[width=6cm]{Rysunki/Rozdzial2/obstacle_detector.png}
	\label{fig:SensorDetekcji}
	\caption[Sensor detekcji obiektów]{Obraz sensora detekcji obiektów.}
\end{figure} 

Sensor radaru jest czujnikiem generuj¹cym sto¿kowy obszar detekcji, w obrêbie którego wykrywane s¹ obiekty znajduj¹ce siê w zasiêgu sensora. Dane radarowe reprezentowane s¹ w postaci dwuwymiarowej mapy punktów, zawieraj¹cej informacje o po³o¿eniu wykrytych elementów oraz ich prêdkoœci wzglêdnej wzglêdem czujnika. Informacje te mog¹ byæ wykorzystywane do analizy ruchu obiektów, okreœlania ich kierunku oraz oceny dynamiki sceny. Ze wzglêdu na zastosowanie wspó³rzêdnych biegunowych, gêstoœæ punktów jest najwiêksza w pobli¿u osi widzenia sensora.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{Rysunki/Rozdzial2/radar_sensor.png}
	\label{fig:przyklad3D}
	\caption[Sensor radaru]{Obraz sensora radaru.}
\end{figure} 

% biblio
%https://carla.readthedocs.io/en/latest/ref_sensors/
%

\subsection{Pogoda}

Pogoda w symulatorze CARLA nie jest reprezentowana jako odrêbna klasa, lecz jako zbiór parametrów œrodowiskowych dostêpnych w œwiecie symulacji. Parametry te obejmuj¹ miêdzy innymi po³o¿enie s³oñca, stopieñ zachmurzenia, si³ê wiatru, mg³ê, deszcz oraz œnieg, co umo¿liwia testowanie algorytmów w szerokim zakresie warunków atmosferycznych. Aby zdefiniowaæ w³asn¹ pogodê, wykorzystuje siê klasê pomocnicz¹ \verb|carla.WeatherParameters|, w której mo¿na ustawiæ odpowiednie wartoœci atrybutów odpowiadaj¹cych za warunki atmosferyczne.

\begin{lstlisting}[style=praca, caption={Przyk³adowy kod ustawiania parametrów pogody w symulatorze CARLA.}]
	weather = carla.WeatherParameters(
	cloudiness=80.0,
	precipitation=30.0,
	sun_altitude_angle=70.0
	)
	world.set_weather(weather)
	print(world.get_weather())
\end{lstlisting}


\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{Rysunki/Rozdzial2/soft_rain_sunset.png}
	\label{fig:lekki_deszcz_zachód}
	\caption[Lekki deszcz o zachodzie s³oñca]{Obraz przedstawiaj¹cy lekki deszcz o zachodzie s³oñca.}
\end{figure} 


Dostêpne s¹ równie¿ gotowe ustawienia pogody, które mo¿na bezpoœrednio zastosowaæ w symulacji jazdy samochodem. Lista zawieraj¹ca przygotowane warunki drogowe znajduje siê w klasie \verb|carla.WeathrrParameters|.

\begin{lstlisting}[style=praca]
	world.set_weather(carla.WeatherParameters.WetCloudySunset)
\end{lstlisting}

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial2/cloudy_night.png}
	\label{fig:zachmurzona_noc}
	\caption[Zachmurzona noc]{Obraz przedstawiaj¹cy zachmurzon¹ noc.}
\end{figure} 

Istnieje równie¿ mo¿liwoœæ dostosowania pogody za pomoc¹ dwóch skryptów dostarczanych w pakiecie symulatora CARLA. 
S¹ to:
\begin{itemize}
	\item \verb|environment.py| (in PythonAPI/util) — Zapewnia dostêp do parametrów pogodowych i oœwietleniowych, dziêki czemu mo¿na je zmieniaæ w czasie rzeczywistym.
	\item \verb|dynamic_weather.py| (in PythonAPI/examples) — W³¹cza okreœlony cykl pogodowy przygotowany przez deweloperów dla ka¿dej mapy CARLA.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial2/hard_rain_noon.png}
	\label{fig:przyklad3D}
	\caption[Mocny deszcz w po³udnie]{Obraz przedstawiaj¹cy mocny deszcz w po³udnie.}
\end{figure} 
%https://carla.readthedocs.io/en/latest/core_world/#weather


\subsection{Oœwietlenie}

Œwiat³a uliczne w³¹czaj¹ siê automatycznie, gdy symulacja przechodzi w tryb nocny. Œwiat³a zosta³y umieszczone przez twórców mapy i s¹ dostêpne jako obiekty carla.Light. W³aœciwoœci takie jak kolor i natê¿enie œwiat³a mog¹ byæ dowolnie zmieniane. Zmienna \verb|light_state| typu \verb|carla.LightState| pozwala ustawiæ je wszystkie w jednym wywo³aniu.
Œwiat³a uliczne s¹ kategoryzowane za pomoc¹ ich atrybutu \verb|light_group|, typu \verb|carla.LightGroup|. Pozwala to na sklasyfikowanie œwiate³ jako œwiat³a uliczne lub œwiat³a budynków. Aby obs³u¿yæ grupy œwiate³ w jednym wywo³aniu, mo¿na pobraæ instancjê \verb|carla.LightManager|.

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial2/street_lights.png}
	\label{fig:przyklad3D}
	\caption[Œwiat³a uliczne]{Obraz przedstawiaj¹cy œwiat³a uliczne.}
\end{figure} 

Œwiat³a pojazdu musz¹ byæ w³¹czane/wy³¹czane przez u¿ytkownika. Ka¿dy pojazd posiada zestaw œwiate³ wymienionych w \verb|carla.VehicleLightState|. Jak dot¹d, nie wszystkie pojazdy maj¹ zintegrowane œwiat³a. Poni¿ej znajduje siê lista tych, które s¹ dostêpne:
\begin{itemize}
	\item \verb|environment.py| (Rowery) — Wszystkie posiadaj¹ przednie i tylne œwiat³o pozycyjne.
	\item \verb|dynamic_weather.py| (Motocykle) — Modele Yamaha i Harley Davidson.
	\item \verb|dynamic_weather.py| (Samochody) - Audi TT, Chevrolet, Dodge (radiowóz), Audi e-tron, Lincoln, Mustang, Tesla 3S, Volkswagen T2 oraz nowi goœcie przybywaj¹cy do CARLA.
\end{itemize}

Œwiat³a pojazdu mog¹ byæ pobierane i aktualizowane w dowolnym momencie za pomoc¹ metod \verb|carla.Vehicle.get_light_state| i \verb|carla.Vehicle.set_light_state|. U¿ywaj¹ one operacji binarnych, aby dostosowaæ ustawienie œwiate³ ~\cite{carla}.

\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{Rysunki/Rozdzial2/vehicle_lights.png}
	\label{fig:przyklad3D}
	\caption[Œwiat³a pojazdu]{Obraz przedstawiaj¹cy œwiat³a pojazdu.}
\end{figure} 

\subsection{Reprezentacja obiektów w przestrzeni 3D i definicja Bounding Box}

Symulator CARLA \cite{carla} oraz Unreal Engine 4 (UE4) stosuj¹ lewoskrêtny uk³ad wspó³rzêdnych kartezjañski z osi¹ Z skierowan¹ w górê (Z-up). Oznacza to:

\begin{itemize}
	\item \textbf{Oœ X}: skierowana do przodu (forward).
	\item \textbf{Oœ Y}: skierowana w prawo (right).
	\item \textbf{Oœ Z}: skierowana w górê (up).
\end{itemize}

Punkt (0, 0, 0) to pocz¹tek uk³adu wspó³rzêdnych, zwany \textit{origin}. Wartoœci dodatnie na osi X oznaczaj¹ ruch do przodu, na osi Y w prawo, a na osi Z w górê \cite{devEpicgames}.

\subsection*{Jednostki miary}

W obu œrodowiskach:

\begin{itemize}
	\item \textbf{1 jednostka (unit)} odpowiada \textbf{1 centymetrowi} w rzeczywistoœci.
	\item \textbf{1 metr} = \textbf{100 jednostek}.
	\item \textbf{1 kilometr} = \textbf{100\,000 jednostek}.
	\item \textbf{1 cal} = \textbf{2,54 jednostki}.
	\item \textbf{1 stopa} = \textbf{30,48 jednostki}.
\end{itemize}

Oznacza to, ¿e zarówno w UE4, jak i w CARLA, 1 jednostka w grze to 1 cm w œwiecie rzeczywistym \cite{devEpicgamesUnits}.

\subsection*{Pozycjonowanie obiektów i punkt (0, 0, 0)}

W UE4 i CARLA:

\begin{itemize}
	\item \textbf{Punkt (0, 0, 0)}: znajduje siê w centrum œwiata gry, zwykle w miejscu, gdzie zaczyna siê scena.
	\item \textbf{Pozycjonowanie obiektów}: odbywa siê poprzez okreœlenie ich lokalizacji wzglêdem tego punktu, np. \texttt{actor.set\_location(FVector(x, y, z))}.
\end{itemize}

	Ka¿dy obiekt dynamiczny (tzw. aktor), taki jak pojazd czy pieszy, posiada swoj¹ pozycjê w globalnym uk³adzie wspó³rzêdnych œwiata, okreœlan¹ wzglêdem punktu $(0,0,0)$ mapy. Kluczowym elementem w procesie detekcji i generowania danych ucz¹cych jest zrozumienie, w jaki sposób fizyczna bry³a obiektu jest reprezentowana numerycznie za pomoc¹ prostopad³oœcianu ograniczaj¹cego, zwanego \textit{bounding box-em} \cite{carla, carla_bounding_boxes}.

\subsubsection{Relacja miêdzy punktem Origin a œrodkiem geometrycznym}

	Ka¿dy aktor w symulacji posiada swój punkt odniesienia, zwany \textit{Origin} lub \textit{Pivot Point}. W przypadku pojazdów punkt ten jest zazwyczaj zlokalizowany na poziomie pod³o¿a, w po³owie odleg³oœci miêdzy osiami kó³, co u³atwia obliczenia fizyki jazdy. Nale¿y jednak wyraŸnie odró¿niæ punkt \textit{Origin} aktora od œrodka jego obrysu (\textit{Bounding Box Center}).

Struktura \texttt{carla.BoundingBox} definiuje geometriê obiektu za pomoc¹ dwóch kluczowych wektorów:
\begin{enumerate}
	\item \texttt{location}: Okreœla przesuniêcie œrodka geometrycznego prostopad³oœcianu wzglêdem punktu \textit{Origin} aktora. Wektor ten $(x_c, y_c, z_c)$ jest wyra¿ony w lokalnym uk³adzie wspó³rzêdnych pojazdu. Przyk³adowo, dla samochodu osobowego œrodek bry³y znajduje siê zazwyczaj wy¿ej (oœ Z > 0) i mo¿e byæ przesuniêty wzd³u¿ osi pod³u¿nej wzglêdem osi kó³.
	\item \texttt{extent}: Jest to wektor $(e_x, e_y, e_z)$ okreœlaj¹cy po³owê wymiarów prostopad³oœcianu wzd³u¿ ka¿dej z osi lokalnych. Oznacza to, ¿e ca³kowite wymiary obiektu wynosz¹ odpowiednio: d³ugoœæ $2 \cdot e_x$, szerokoœæ $2 \cdot e_y$ oraz wysokoœæ $2 \cdot e_z$.
\end{enumerate}

Wspó³rzêdne wierzcho³ków bounding boxa nie s¹ przechowywane wprost, lecz s¹ obliczane dynamicznie na podstawie jego œrodka oraz wektora \texttt{extent}. Dla lokalnego uk³adu odniesienia, wierzcho³ki $V_{local}$ zdefiniowane s¹ jako kombinacje dodawania i odejmowania wartoœci \texttt{extent} od œrodka \texttt{location} \cite{carla_bounding_boxes}.

%\begin{figure}[H]
%	\centering
%	% Tutaj wstaw grafikê ilustruj¹c¹ wektory extent i relacjê origin-center
%	% Nazwij plik np. bbox_vectors_diagram.png
%	\includegraphics[width=0.8\textwidth]{Rysunki/Rozdzial2/bbox_vectors_diagram.png}
%	\caption{Schemat definicji Bounding Boxa: relacja miêdzy punktem Origin aktora, œrodkiem Bounding Boxa (Center) oraz wektorem Extent okreœlaj¹cym wymiary bry³y.}
%	\label{fig:bbox_diagram}
%\end{figure}

\subsubsection{Transformacja do uk³adu wspó³rzêdnych œwiata}

Aby wyznaczyæ po³o¿enie wierzcho³ków bounding boxa w globalnej przestrzeni 3D symulacji, konieczne jest wykonanie transformacji macierzowej. Proces ten uwzglêdnia aktualn¹ pozycjê i rotacjê pojazdu na mapie. Wykorzystywana jest do tego macierz transformacji $M_{actor}$, która sk³ada siê z translacji (pozycji aktora wzglêdem punktu 0,0,0 œwiata) oraz rotacji (k¹tów \textit{roll, pitch, yaw}).

Po³o¿enie dowolnego wierzcho³ka $V_{world}$ w przestrzeni œwiata obliczane jest zgodnie z zale¿noœci¹:
\begin{equation}
	V_{world} = M_{actor} \cdot V_{local}
\end{equation}
Gdzie $V_{local}$ to wspó³rzêdna wierzcho³ka w uk³adzie lokalnym pojazdu (uwzglêdniaj¹ca przesuniêcie \texttt{location} i wymiar \texttt{extent}). Operacja ta pozwala na precyzyjne umiejscowienie obrysu pojazdu w œwiecie 3D, niezale¿nie od jego orientacji czy pochylenia wynikaj¹cego z fizyki zawieszenia \cite{carla}.

\subsubsection{Rzutowanie z przestrzeni 3D na p³aszczyznê obrazu 2D}

Ostatnim etapem, niezbêdnym do wygenerowania danych referencyjnych dla algorytmu YOLO (tzw. \textit{ground truth}), jest rzutowanie trójwymiarowych wierzcho³ków bounding boxa na dwuwymiarow¹ p³aszczyznê obrazu z kamery. Proces ten, realizowany m.in. w skrypcie \texttt{bounding\_boxes.py}, wymaga znajomoœci parametrów wewnêtrznych i zewnêtrznych kamery.

Rzutowanie odbywa siê w dwóch krokach:
\begin{enumerate}
	\item \textbf{Transformacja Œwiat $\rightarrow$ Kamera:} Punkty ze œwiata 3D s¹ przeliczane do uk³adu wspó³rzêdnych kamery przy u¿yciu macierzy \textit{World-to-Camera} (bêd¹cej odwrotnoœci¹ transformacji kamery w œwiecie).
	\item \textbf{Projekcja perspektywiczna:} Punkty z uk³adu kamery s¹ rzutowane na p³aszczyznê obrazu przy u¿yciu macierzy kalibracji $K$ (macierz parametrów wewnêtrznych), która zale¿y od rozdzielczoœci obrazu oraz k¹ta widzenia (FOV).
\end{enumerate}

Wzór opisuj¹cy projekcjê punktu $P_{3D} = [x, y, z]^T$ na punkt obrazu $p_{2D} = [u, v]^T$ (we wspó³rzêdnych jednorodnych) przyjmuje postaæ:
\begin{equation}
	s \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = K \cdot [R | t] \cdot \begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix}
\end{equation}
Gdzie $K$ to macierz wewnêtrzna kamery, $[R|t]$ to macierz transformacji z uk³adu œwiata do uk³adu kamery, a $s$ to wspó³czynnik skaluj¹cy. Ostateczny prostok¹t 2D (bbox 2D) wyznaczany jest poprzez znalezienie minimalnych i maksymalnych wartoœci wspó³rzêdnych $(u, v)$ spoœród wszystkich 8 rzutowanych wierzcho³ków bry³y 3D. Takie podejœcie gwarantuje, ¿e wygenerowany obrys 2D w pe³ni obejmuje widoczny obiekt na zdjêciu \cite{carla_bounding_boxes}.

\section{Instalacja symulatora CARLA}

\subsection{Wymagania sprzêtowe}

Rekomendowana specyfikacja komputera dla najnowszej wersji symulatora CARLA  0.9.13 prezentuje siê nastêpuj¹co:
\begin{itemize}
	\item \verb|Procesor| Intel i7 gen 9th - 11th / Intel i9 gen 9th - 11th / AMD ryzen 7 / AMD ryzen 9
	\item \verb|Iloœæ pamiêci RAM| +16 GB pamiêci RAM
	\item \verb|Iloœæ miejsca na dysku| 130GB
	\item \verb|Karta graficzna| najlepiej z 6GB lub 8GB pamiêci  VRAM np. NVIDIA RTX 2070 / NVIDIA RTX 2080 / NVIDIA RTX 3070, NVIDIA RTX 3080 lub nowsze
	\item \verb|System operacyjny| Ubuntu 18.04/ 20.04/ 22.04/ Windows 10
\end{itemize}

\subsection{Instalacja i konfiguracja dla systemu Ubuntu}

Pierwszym etapem instalacji CARLA jest przygotowanie systemu. W pierwszej kolejnoœci system powinien zostaæ zaktualizowany, a niezbêdne pakiety zainstalowane:

\begin{lstlisting}[caption={Aktualizacja systemu}]
	sudo apt update
	sudo apt upgrade
\end{lstlisting}

Nastêpnie musz¹ zostaæ zainstalowane zale¿noœci wymagane przez CARLA:

\begin{lstlisting}[caption={Instalacja zale¿noœci}]
	sudo apt install build-essential clang cmake git libcurl4-openssl-dev libssl-dev \
	libsqlite3-dev libudev-dev pkg-config python3-dev python3-pip \
	python3-setuptools python3-wheel qt5-qmake qtbase5-dev libqt5core5a \
	libqt5gui5 libqt5widgets5 libprotobuf-dev protobuf-compiler \
	libsdl2-dev libpng-dev libjpeg-dev libtiff-dev libgtk-3-dev \
	libassimp-dev libblas-dev liblapack-dev libboost-all-dev \
	libopenblas-dev libatlas-base-dev libeigen3-dev
\end{lstlisting}

Dodatkowo, konieczne jest zainstalowanie jêzyka Python oraz mened¿era pakietów PIP:

\begin{lstlisting}[caption={Instalacja Pythona i PIP}]
	sudo apt install python3 python3-pip
	python3 -m pip install --upgrade pip
\end{lstlisting}

\subsection{Instalacja Unreal Engine}

Ze wzglêdu na to, ¿e CARLA jest oparta na silniku Unreal Engine, konieczne jest jego zainstalowanie. Proces ten mo¿e zostaæ przeprowadzony za pomoc¹ Epic Games Launcher ~\cite{unreal-engine}.

\begin{enumerate}
	\item Nale¿y za³o¿yæ konto na stronie \texttt{https://www.epicgames.com/}.
	\item Nastêpnie nale¿y pobraæ i zainstalowaæ Epic Games Launcher.
	\item Po instalacji wymagane jest zalogowanie siê i przejœcie do zak³adki \texttt{Library}.
	\item Nale¿y wybraæ odpowiedni¹ wersjê Unreal Engine (zalecana 4.27 lub nowsza) i przeprowadziæ instalacjê.
	\item Po zakoñczeniu instalacji Unreal Engine powinien zostaæ uruchomiony za poœrednictwem Epic Games Launcher.
\end{enumerate}

\subsection{Pobranie Ÿróde³ CARLA i kompilacja}

Aby pobraæ CARLA, repozytorium musi zostaæ sklonowane z GitHub:

\begin{lstlisting}[caption={Pobieranie Ÿróde³ CARLA}]
	cd ~
	git clone https://github.com/carla-simulator/carla.git
	cd carla
\end{lstlisting}

Kolejnym krokiem jest przejœcie do katalogu zawieraj¹cego projekt Unreal Engine dla CARLA:

\begin{lstlisting}[caption={Przechodzenie do folderu Unreal Engine dla CARLA}]
	cd Unreal/CarlaUE4
\end{lstlisting}

Nastêpnie konieczne jest uruchomienie Unreal Engine w celu konfiguracji projektu:

\begin{lstlisting}[caption={Uruchamianie Unreal Engine}]
	~/UnrealEngine/Engine/Binaries/Linux/UE4Editor CarlaUE4.uproject
\end{lstlisting}

Po uruchomieniu Unreal Engine nale¿y wybraæ opcjê \texttt{Generate Visual Studio project files} i zamkn¹æ edytor.

W kolejnym kroku wymagane jest powrócenie do terminala i przeprowadzenie kompilacji:

\begin{lstlisting}[caption={Kompilacja CARLA}]
	make
\end{lstlisting}

\subsection{Uruchomienie symulatora CARLA}

Po zakoñczeniu kompilacji symulator CARLA mo¿e zostaæ uruchomiony:

\begin{lstlisting}[caption={Uruchamianie CARLA}]
	cd ~/carla/Unreal/CarlaUE4/Binaries/Linux
	./CarlaUE4
\end{lstlisting}

Po pomyœlnym uruchomieniu symulatora powinien ukazaæ siê obraz mapy domyœlnej widocznym na Rys. \ref{fig:mapaTown01}. Korzystanie z API CARLA w Pythonie Mo¿na rozpocz¹æ poprzez instalacjê odpowiednich pakietów:

\begin{lstlisting}[caption={Instalacja zale¿noœci Pythona}]
	pip3 install carla
\end{lstlisting}

Aby zweryfikowaæ poprawnoœæ dzia³ania, mo¿na uruchomiæ przyk³adowy skrypt Pythona:

\begin{lstlisting}[caption={Uruchamianie przyk³adowego skryptu}]
	cd ~/carla/PythonAPI/examples
	python spawn_npc.py
\end{lstlisting}

Je¿eli wszystkie kroki zosta³y wykonane poprawnie, symulator CARLA powinien dzia³aæ, a skrypt Python powinien uruchomiæ siê bez problemów.

\section{Skrypt do manualnego sterowania w symulatorze CARLA}
\label{sec:manual_control}

Skrypt \verb|manual_control.py| dostarczany wraz z symulatorem CARLA
umo¿liwia rêczne sterowanie pojazdem z poziomu klawiatury oraz podgl¹d
obrazu z kamer w czasie rzeczywistym. Stanowi on punkt wyjœcia do dalszej
integracji z zewnêtrznymi algorytmami przetwarzania danych, takimi jak
systemy detekcji obiektów.

\subsection{Struktura skryptu}
Plik \verb|manual_control.py| zosta³ zaprojektowany modularnie i obejmuje kilka kluczowych sekcji, z których ka¿da realizuje specyficzny etap dzia³ania aplikacji:
\begin{itemize}
	\item \textbf{Importowanie bibliotek oraz konfiguracja klienta CARLA} – inicjalizacja po³¹czenia z serwerem symulacji oraz za³adowanie niezbêdnych bibliotek zewnêtrznych, takich jak \verb|pygame| czy \verb|carla|.
	\item \textbf{Inicjalizacja œrodowiska i pojazdu} – stworzenie œwiata symulacyjnego, pojazdu oraz pod³¹czenie odpowiednich czujników.
	\item \textbf{Obs³uga wejœcia u¿ytkownika} – monitorowanie urz¹dzeñ wejœciowych (klawiatura, mysz) za pomoc¹ biblioteki \verb|pygame|.
	\item \textbf{G³ówna pêtla gry (game loop)} – ci¹g³e przetwarzanie zdarzeñ, aktualizacja stanu œwiata oraz renderowanie obrazu.
	\item \textbf{Zakoñczenie i czyszczenie zasobów} – prawid³owe usuniêcie wszystkich aktorów i zwolnienie zasobów systemowych.
\end{itemize}
Ka¿dy z tych etapów pe³ni fundamentaln¹ rolê w zapewnieniu poprawnej i wydajnej pracy symulacji.

\subsection{Logika dzia³ania programu \texttt{manual\_control.py}}
Program \verb|manual_control.py| realizuje pe³ny cykl ¿ycia aplikacji symulacyjnej: od inicjalizacji œrodowiska, przez obs³ugê sterowania rêcznego, a¿ po prawid³owe zakoñczenie symulacji. Jego struktura zosta³a przedstawiona na schemacie blokowym (rysunek \ref{fig:manual_control_diagram}).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{Rysunki/Rozdzial2/diagram_manual_control.png}
	\caption{Schemat blokowy dzia³ania pliku \texttt{manual\_control.py}}
	\label{fig:manual_control_diagram}
\end{figure}

G³ówne komponenty systemu:
\begin{itemize}
	\item \textbf{Main Loop} – odpowiada za przetwarzanie zdarzeñ wejœciowych, aktualizacjê stanu œwiata oraz renderowanie grafiki.
	\item \textbf{CameraManager} – umo¿liwia zarz¹dzanie widokami kamer i przetwarzanie danych z czujników wizualnych.
	\item \textbf{World} – zarz¹dza ca³ym œwiatem symulacji, w tym pojazdem u¿ytkownika, pogod¹ oraz czujnikami.
	\item \textbf{KeyboardControl} – przetwarza sygna³y z klawiatury i przek³ada je na konkretne komendy steruj¹ce pojazdem.
	\item \textbf{Sensors} – obejmuje wszystkie dostêpne czujniki w symulatorze (kamera RGB, czujniki kolizji, lane invasion sensor, GNSS, IMU, radar).
	\item \textbf{HUD (Heads-Up Display)} – odpowiedzialny za prezentowanie informacji o stanie symulacji w formie graficznej.
\end{itemize}

Komunikacja miêdzy tymi komponentami odbywa siê w sposób asynchroniczny, zapewniaj¹c p³ynne dzia³anie symulacji oraz wysok¹ responsywnoœæ interfejsu u¿ytkownika.
\subsection{G³ówna pêtla gry - \texttt{game\_loop()}}
\label{subsec:game_loop}

G³ówna logika dzia³ania programu \verb|manual_control.py| zosta³a
zaimplementowana w funkcji \verb|game_loop()|, która pe³ni rolê pêtli gry
(\textit{game loop}). W ka¿dej iteracji pêtla ta odczytuje stan wejœcia
od u¿ytkownika, aktualizuje sterowanie pojazdem oraz kontroluje tempo
symulacji, zapewniaj¹c p³ynne dzia³anie programu.

\begin{lstlisting}[style=pythonColor, emph={Client, get\_world, VehicleControl, Clock, get\_pressed, parse\_control\_input, get\_actors, apply\_control, tick}, caption={Przyk³adowy kod pêtli gry -- \texttt{game\_loop()}}]
	def main():
	client = carla.Client('localhost', 2000)        # Inicjalizacja klienta CARLA
	world = client.get_world()                      # Pobranie œwiata symulacji
	control = carla.VehicleControl()                # Obiekt sterowania pojazdem
	
	pygame.init()                                   # Inicjalizacja pygame
	clock = pygame.time.Clock()                     # Zegar do kontrolowania FPS
	
	try:
	while True:
	keys = pygame.key.get_pressed()         # Pobieranie stanu klawiszy
	control = parse_control_input(keys)     # Analiza wejœcia od u¿ytkownika
	
	vehicle = world.get_actors().filter('vehicle.*')[0]  # Pobranie pojazdu
	vehicle.apply_control(control)          # Zastosowanie sterowania
	
	clock.tick(30)                          # Ograniczenie liczby klatek na sekundê
	except KeyboardInterrupt:
	pass
\end{lstlisting}

Dzia³anie pêtli \verb|game_loop()| mo¿na opisaæ w nastêpuj¹cych krokach:
\begin{enumerate}
	\item \textbf{Inicjalizacja komponentów}: tworzeni s¹ klient CARLA,
	obiekt œwiata \verb|world|, obiekt sterowania \verb|VehicleControl|
	oraz zegar \verb|Clock|, a biblioteka \verb|pygame| jest przygotowywana
	do obs³ugi wejœcia z klawiatury.
	
	\item \textbf{Pobranie stanu klawiszy}: funkcja
	\verb|pygame.key.get_pressed()| odczytuje aktualny stan wszystkich
	klawiszy na klawiaturze.
	
	\item \textbf{Przetwarzanie wejœcia u¿ytkownika}: funkcja
	\verb|parse_control_input(keys)| interpretuje wciœniête klawisze (np.
	\verb|W|, \verb|S|, \verb|A|, \verb|D|, spacja) i na tej podstawie ustawia
	parametry \verb|throttle|, \verb|brake| oraz \verb|steer| w obiekcie
	\verb|carla.VehicleControl|.
	
	\item \textbf{Zastosowanie sterowania}: skonfigurowany obiekt
	\verb|VehicleControl| przekazywany jest do metody
	\verb|vehicle.apply_control(control)|, co powoduje wykonanie odpowiedniej
	akcji przez pojazd (przyspieszenie, hamowanie, skrêt).
	
	\item \textbf{Kontrola liczby klatek na sekundê}: funkcja
	\verb|clock.tick(30)| ogranicza czêstotliwoœæ wykonywania pêtli do
	30 iteracji na sekundê, co zapewnia p³ynnoœæ symulacji i stabilne
	warunki testowania.
\end{enumerate}

\subsection{Tryby asynchroniczny i synchroniczny w symulatorze CARLA}
\label{subsec:async_sync_modes}

Symulator CARLA umo¿liwia uruchamianie œrodowiska w dwóch podstawowych trybach: asynchronicznym oraz synchronicznym. W trybie asynchronicznym serwer symulacji samodzielnie aktualizuje œwiat z w³asn¹ czêstotliwoœci¹, niezale¿nie od dzia³añ klienta. W trybie synchronicznym ka¿da klatka symulacji generowana jest dopiero po wywo³aniu odpowiedniej metody po stronie klienta, co pozwala œciœle zsynchronizowaæ stan œwiata z danymi z czujników.

\subsubsection*{Tryb asynchroniczny (domyœlny)}

Tryb asynchroniczny jest ustawieniem domyœlnym po uruchomieniu symulatora CARLA i pod³¹czeniu klienta, takiego jak skrypt \verb|manual_control.py|. Serwer aktualizuje fizykê, po³o¿enie aktorów oraz dane z czujników z maksymaln¹ mo¿liw¹ czêstotliwoœci¹, niezale¿nie od tego, jak szybko klient przetwarza otrzymywane informacje. Pêtla gry po stronie klienta mo¿e skupiæ siê na obs³udze wejœcia u¿ytkownika z klawiatury, aktualizacji interfejsu HUD oraz renderowaniu obrazu, podczas gdy serwer w tle nieprzerwanie prowadzi symulacjê. Wad¹ tego trybu jest brak gwarancji, ¿e obraz z kamer i inne pomiary bêd¹ dok³adnie odpowiada³y temu samemu krokowi czasowemu, co utrudnia precyzyjne gromadzenie danych do trenowania i ewaluacji algorytmów detekcji obiektów.

W pocz¹tkowej fazie przeprowadzanych eksperymentów tryb asynchroniczny okaza³ siê niewystarczaj¹cy przy testowaniu algorytmu YOLO w czasie rzeczywistym. Serwer symulatora generowa³ kolejne klatki szybciej, ni¿ klient by³ w stanie je przetwarzaæ, co prowadzi³o do rozjechania siê czasowego danych: ramki detekcji odnosi³y siê do poprzednich stanów œwiata, a nie do aktualnie renderowanego obrazu. W efekcie obrysy obiektów, takich jak piesi czy pojazdy, by³y rysowane z wyraŸnym opóŸnieniem, czêsto w miejscach, w których obiekt znajdowa³ siê jedynie w poprzednich klatkach. Problem ten zosta³ wyeliminowany dopiero po prze³¹czeniu symulacji w tryb synchroniczny, w którym ka¿da klatka obrazu jest jednoznacznie powi¹zana z odpowiadaj¹cymi jej wynikami detekcji.

%\begin{figure}[H]
%	\centering
%	% \includegraphics[width=\textwidth]{RysunkiRozdzial2_async_mode.png}
%	\caption{Widok symulatora CARLA w domyœlnym trybie asynchronicznym.}
%	\label{fig:carla_async_mode}
%\end{figure}

\subsubsection*{Tryb synchroniczny i uruchamianie z poziomu terminala}

Tryb synchroniczny zapewnia deterministyczne i powtarzalne dzia³anie symulacji, w którym ka¿da klatka jest jednoznacznie powi¹zana z kompletem danych z czujników. Klient wywo³uje metodê \verb|world.tick()| tylko wtedy, gdy jest gotowy na pobranie kolejnej próbki danych, co pozwala œciœle zsynchronizowaæ stan œwiata z obrazami z kamer, chmurami punktów LIDAR oraz innymi pomiarami. Jest to szczególnie istotne przy przygotowywaniu zbiorów danych dla modeli detekcji obiektów, takich jak YOLO, gdzie dla ka¿dej klatki obrazu wymagane s¹ dok³adnie odpowiadaj¹ce jej adnotacje.

Prze³¹czenie symulacji w tryb synchroniczny wymaga jawnego wymuszenia tego ustawienia po stronie klienta. W kodzie odbywa siê to poprzez zmianê konfiguracji œwiata:

\begin{lstlisting}[style=pythonColor, emph={get\_settings, synchronous\_mode, fixed\_delta\_seconds, apply\_settings}, caption={Konfiguracja trybu synchronicznego w kodzie klienta}]
	settings = world.get_settings()
	settings.synchronous_mode = True
	settings.fixed_delta_seconds = 0.05  # np. 20 FPS
	world.apply_settings(settings)
\end{lstlisting}

Skrypt \verb|manual_control.py| udostêpnia w tym celu prze³¹cznik w linii poleceñ, który aktywuje tryb synchroniczny. Uruchomienie klienta z poziomu terminala mo¿e wygl¹daæ nastêpuj¹co:

\begin{lstlisting}[caption={Uruchomienie skryptu manual\_control.py w trybie synchronicznym (Linux)}]
	python3 manual_control.py --sync
\end{lstlisting}

lub w systemie Windows:

\begin{lstlisting}[caption={Uruchomienie skryptu manual\_control.py w trybie synchronicznym (Windows)}]
	python manual_control.py --sync
\end{lstlisting}

Podanie parametru \verb|--sync| powoduje, ¿e w kodzie klienta ustawiany jest tryb synchroniczny oraz ewentualny sta³y krok czasowy, a nastêpnie wywo³ywana jest metoda \verb|world.apply_settings(settings)|, co prze³¹cza serwer w tryb synchroniczny. Zalet¹ takiej organizacji jest mo¿liwoœæ œcis³ego sprzê¿enia symulacji z zewnêtrznymi algorytmami, które mog¹ przetworzyæ dane z bie¿¹cej klatki i dopiero potem zainicjowaæ przejœcie do nastêpnego kroku czasowego. Wad¹ jest natomiast to, ¿e ca³kowita prêdkoœæ symulacji zale¿y od wydajnoœci klienta – im d³u¿ej trwa przetwarzanie danych, tym wolniej generowane s¹ kolejne klatki.

%\begin{figure}[H]
%	\centering
%	% \includegraphics[width=\textwidth]{RysunkiRozdzial2_sync_mode.png}
%	\caption{Widok symulatora CARLA w wymuszonym trybie synchronicznym.}
%	\label{fig:carla_sync_mode}
%\end{figure}

\subsection{Znaczenie dla integracji z algorytmami zewnêtrznymi}

Zastosowanie trybu synchronicznego jest szczególnie istotne w kontekœcie integracji z zewnêtrznymi algorytmami analizy danych, np. systemami opartymi o YOLO. W takich przypadkach przetwarzanie obrazu musi byæ zgodne ze stanem symulacji, aby wykrywane obiekty odpowiada³y ich rzeczywistej pozycji i prêdkoœci. Brak synchronizacji mo¿e prowadziæ do b³êdnych detekcji i zaburzenia procesu wnioskowania.

\subsection{Sterowanie pojazdem z poziomu klawiatury}

Jednym z g³ównych elementów interakcji u¿ytkownika z pojazdem w symulatorze CARLA jest sterowanie przy pomocy klawiatury. W tym celu skrypt \texttt{manual\_control.py} wykorzystuje bibliotekê \texttt{pygame}, która pozwala na obs³ugê wejœcia u¿ytkownika i monitorowanie stanu klawiszy. G³ówna funkcja odpowiedzialna za to zadanie to \texttt{parse\_control\_input()}, która analizuje naciœniête klawisze i na ich podstawie dostosowuje parametry sterowania pojazdem.

\begin{lstlisting}[style=pythonColor, emph={parse\_control\_input, get\_pressed, VehicleControl}, caption={Przyk³adowy kod obs³ugi sterowania pojazdem z klawiatury}]
	import pygame
	
	def parse_control_input():
	keys = pygame.key.get_pressed()       # Pobranie stanu klawiszy
	control = carla.VehicleControl()      # Obiekt sterowania pojazdem
	
	if keys[pygame.K_w]:                  # Przyspieszanie (gaz)
	control.throttle = 1.0
	if keys[pygame.K_s]:                  # Hamowanie
	control.brake = 1.0
	if keys[pygame.K_a]:                  # Skrêt w lewo
	control.steer = -1.0
	if keys[pygame.K_d]:                  # Skrêt w prawo
	control.steer = 1.0
	
	return control                        # Zwrócenie obiektu sterowania
\end{lstlisting}

\noindent
Dzia³anie funkcji mo¿na podsumowaæ nastêpuj¹co:
\begin{itemize}
	\item \textbf{Pobieranie stanu klawiszy} – funkcja \verb|pygame.key.get_pressed()| zwraca tablicê wartoœci logicznych dla wszystkich klawiszy, umo¿liwiaj¹c detekcjê aktualnie wciœniêtych przycisków.
	\item \textbf{Tworzenie obiektu sterowania} – obiekt \verb|carla.VehicleControl| przechowuje parametry steruj¹ce pojazdem, takie jak \verb|throttle| (przyspieszenie), \verb|brake| (hamowanie) oraz \verb|steer| (k¹t skrêtu).
	\item \textbf{Ustawianie parametrów} – na podstawie wciœniêtych klawiszy przypisywane s¹ odpowiednie wartoœci do w³aœciwoœci \verb|throttle|, \verb|brake| i \verb|steer|, np. klawisz \verb|W| powoduje przyspieszanie pojazdu, a \verb|S| uruchamia hamowanie.
	\item \textbf{Zwrócenie obiektu sterowania} – po przetworzeniu stanu klawiatury funkcja zwraca skonfigurowany obiekt \verb|control|, który nastêpnie jest przekazywany do metody \verb|vehicle.apply_control(control)| odpowiedzialnej za wykonanie manewru w œwiecie symulacyjnym.
\end{itemize}

\subsection{Czyszczenie zasobów (Cleanup)}
Po zakoñczeniu dzia³ania symulacji niezbêdne jest prawid³owe usuniêcie wszystkich aktorów, aby zwolniæ pamiêæ oraz unikn¹æ potencjalnych b³êdów.

Proces ten obejmuje:
\begin{itemize}
	\item \textbf{Pobranie aktorów} – funkcja \verb|world.get_actors()| zwraca wszystkie aktywne obiekty w œwiecie.
	\item \textbf{Zniszczenie aktorów} – poprzez iteracyjne wywo³ywanie metody \verb|destroy()| na ka¿dym z aktorów.
\end{itemize}

Przyk³adowy kod funkcji czyszcz¹cej:

\begin{lstlisting}[style=pythonColor, emph={cleanup, get\_actors, destroy}, caption={Kod funkcji odpowiedzialnej za czyszczenie zasobów}]
	def cleanup(world):
	actors = world.get_actors()
	for actor in actors:
	actor.destroy()
\end{lstlisting}

Poprawne czyszczenie zasobów zapewnia optymaln¹ wydajnoœæ symulacji i umo¿liwia jej wielokrotne uruchamianie bez ryzyka narastania b³êdów pamiêciowych.